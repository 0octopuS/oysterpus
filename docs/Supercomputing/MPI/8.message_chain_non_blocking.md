# 8. Non-blocking communication and persistent communication

!!! abstract "Take Away" 
    - `MPI_Request`: to store the information of communication
    - `MPI_Isend(..., *MPI_Request)`: non-blocking send
    - `MPI_Irecv(..., *MPI_Request)`: non-blocking recv, there is no `MPI_Status` because `MPI_Irecv` ends before communication ends. In this stage there is no `status`.
    - `MPI_Send_init/MPI_Recv_init` buddle the parameter to a `request`, using it can start the persistent communication.
    - `MPI_Wait(*MPI_Request, *MPI_Status)`: wait for 1 communication ending.
    - `MPI_Waitall(count, *MPI_Request, *MPI_Status);` wait for all communications ending.

## 8.1 Non-blocking

`MPI_Isend/Irecv` provide non-blocking send and recv. The actions behind:

- Set up sending and receiving
- While communication occurs, computation can **continue** (of course, computations that do not depend on the information being communicated);
- Wait for communication to complete (communication happens in the **background**).

=== ":octicons-terminal-16: Code"
    - Create `MPI_Status` and `MPI_Request` array.
    - Use `MPI_Isend` and `MPI_Irecv` for non-blocking send/recv. The last parameter is `*MPI_Request`.
    - Collect status by `MPI_Waitall`.
    ```cpp
    MPI_Status status[2];
    MPI_Request request[2];
    MPI_Isend(message.data(), size, MPI_INT, destination, myid+1, MPI_COMM_WORLD, &request[0]);
    MPI_Irecv(receiveBuffer.data(), size, MPI_INT, source, myid, MPI_COMM_WORLD, &request[1]);
    MPI_Waitall(2, request, status);
    ```
=== ":octicons-code-review-16: Puhti Result(4)"
    ```sh
    Sender: 0. Sent elements: 10000000. Tag: 1. Receiver: 1
    Receiver: 0. first element -1.
    Sender: 1. Sent elements: 10000000. Tag: 2. Receiver: 2
    Receiver: 1. first element 0.
    Sender: 2. Sent elements: 10000000. Tag: 3. Receiver: 3
    Receiver: 2. first element 1.
    Sender: 3. Sent elements: 10000000. Tag: 4. Receiver: -2
    Receiver: 3. first element 2.
    Time elapsed in rank  0:  0.009
    Time elapsed in rank  1:  0.009
    Time elapsed in rank  2:  0.037
    Time elapsed in rank  3:  0.009
    ```

!!! question "Investigate the timings with different numbers of MPI tasks (e.g. 2, 4, 8, 16, ...)"
    - 2 tasks: Time elapsed in rank  0:  0.009
    - 4 tasks: Time elapsed in rank  0:  0.009
    - 8 tasks: Time elapsed in rank  0:  0.021
    - 16 tasks: Time elapsed in rank  0:  0.014


## 8.2 persistent

A communication is performed continuously with the **same parameters** in an inner loop of parallel computation. In this case, the communication can be optimized by **bundling these communication parameters** once into a persistent communication request, and then continuously using this request to initialize and complete messages.

=== ":octicons-terminal-16: Code"
    ```cpp
    MPI_Send_init(message.data(), size, MPI_INT, destination, myid+1, MPI_COMM_WORLD, &request[0]);
    MPI_Recv_init(receiveBuffer.data(), size, MPI_INT, source, myid, MPI_COMM_WORLD, &request[1]);
    MPI_Start( &request[0]);
    MPI_Start( &request[1]);
    MPI_Waitall(2, request, status);
    ```