# 6. Heat equation

!!! abstract "Take Away" 
    


Benchmark result by serial code

| command(./heat) | rows/cols | time steps | temperature         | Time      |
| --------------- | --------- | ---------- | ------------------- | --------- |
|                 | 2000/2000 | 500        | 59.763305/59.281239 | 3.757034  |
| bottle.dat      | 200/200   | 500        | 86.513850/86.560479 | 0.035772  |
| bottle.dat 1000 | 200/200   | 1000       | 86.513850/86.726629 | 0.085675  |
| 4000 8000 1000  | 4000/8000 | 1000       | 62.381996/62.122529 | 60.536838 |


## 6.1 First step

=== ":octicons-terminal-16: heat.hpp/ParallelData()"
    ```cpp
    MPI_Comm_size( MPI_COMM_WORLD, &size);
    MPI_Comm_rank( MPI_COMM_WORLD , &rank);
    nup = (rank == 0) ? MPI_PROC_NULL : rank - 1;
    ndown = (rank == size -1) ? MPI_PROC_NULL : rank +1;
    ```
=== ":octicons-terminal-16: core.cpp/exchange()"
    ```cpp
    double* sbuf = field.temperature.data(1, 0);
    double* rbuf = field.temperature.data(field.nx + 1, 0);

    // Send to up, receive from down
    MPI_Send( sbuf , field.ny + 2 , MPI_DOUBLE , parallel.nup , parallel.nup + 100, MPI_COMM_WORLD);
    MPI_Recv( rbuf , field.ny + 2 , MPI_DOUBLE , parallel.ndown , parallel.rank + 100, MPI_COMM_WORLD,MPI_STATUS_IGNORE);

    // Send to down, receive from up
    sbuf = field.temperature.data(field.nx, 0);
    rbuf = field.temperature.data();
    MPI_Send( sbuf , field.ny +2 , MPI_DOUBLE , parallel.ndown , parallel.ndown + 200, MPI_COMM_WORLD);
    MPI_Recv( rbuf , field.ny + 2 , MPI_DOUBLE , parallel.nup , parallel.rank + 200, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
    ```

The result using 4 cores, all the temperature result is correct. The time decrease more when the scale is large.

| command(./heat) | temperature         | Time      | Serial Time |
| --------------- | ------------------- | --------- | ----------- |
|                 | 59.763305/59.281239 | 1.609080  | 3.757034    |
| bottle.dat      | 86.513850/86.560479 | 0.033550  | 0.035772    |
| bottle.dat 1000 | 86.513850/86.726629 | 0.074306  | 0.085675    |
| 4000 8000 1000  | 62.381996/62.122529 | 26.286924 | 60.536838   |


## 6.2 Sendrecv

=== ":octicons-terminal-16: core.cpp/exchange()"
    ```cpp
    MPI_Sendrecv( sbuf , field.ny + 2 , MPI_DOUBLE ,parallel.nup  , parallel.nup + 100, rbuf , field.ny + 2, MPI_DOUBLE ,parallel.ndown , parallel.rank + 100 , MPI_COMM_WORLD,MPI_STATUS_IGNORE);
    MPI_Sendrecv( sbuf , field.ny + 2 , MPI_DOUBLE ,parallel.ndown  , parallel.ndown + 200, rbuf , field.ny + 2, MPI_DOUBLE ,parallel.nup , parallel.rank + 200 , MPI_COMM_WORLD,MPI_STATUS_IGNORE);
    ```

Use `Sendrecv`, time is slightly higher than `send` and `recv`.

| command(./heat) | Time      | Serial Time | Send&Recv Time |
| --------------- | --------- | ----------- | -------------- |
|                 | 1.635686  | 3.757034    | 1.609080       |
| bottle.dat      | 0.034436  | 0.035772    | 0.033550       |
| bottle.dat 1000 | 0.074368  | 0.085675    | 0.074306       |
| 4000 8000 1000  | 26.322003 | 60.536838   | 26.286924      |

## 6.3 Collective communication

=== ":octicons-terminal-16: utilities.cpp/average()"
    - reduce result from all processes, and distributed them back.
    - the operation is `MPI_SUM`.
    ```cpp
    MPI_Allreduce( &local_average , &average , 1 , MPI_DOUBLE , MPI_SUM , MPI_COMM_WORLD);
    ```

The middle two have a obvious time increase.

| command(./heat) | Sendrecv+Allreduce | Serial Time | Send&Recv Time | Sendrecv Time |
| --------------- | ------------------ | ----------- | -------------- | ------------- |
|                 | 1.888718           | 3.757034    | 1.609080       | 1.635686      |
| bottle.dat      | 0.270826           | 0.035772    | 0.033550       | 0.034436      |
| bottle.dat 1000 | 0.400864           | 0.085675    | 0.074306       | 0.074368      |
| 4000 8000 1000  | 29.689776          | 60.536838   | 26.286924      | 26.322003     |

## 6.4 Non-blocking

=== ":octicons-terminal-16: Code"
    ```cpp
    MPI_Request request[4];
    MPI_Status status[4];
    MPI_Isend( sbuf , field.ny + 2 , MPI_DOUBLE , parallel.nup , parallel.nup + 100, MPI_COMM_WORLD, & request[0]);
    MPI_Irecv( rbuf , field.ny + 2 , MPI_DOUBLE , parallel.ndown , parallel.rank + 100, MPI_COMM_WORLD,& request[1]);
    // ... 
    MPI_Isend( sbuf , field.ny +2 , MPI_DOUBLE , parallel.ndown , parallel.ndown + 200, MPI_COMM_WORLD,& request[2]);
    MPI_Irecv( rbuf , field.ny + 2 , MPI_DOUBLE , parallel.nup , parallel.rank + 200, MPI_COMM_WORLD, & request[3]);
    MPI_Waitall( 4 , request , status);
    ```

## 6.5 Using Cartesian communicator


## 6.6 2D decomposition