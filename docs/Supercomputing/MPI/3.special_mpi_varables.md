# 3. Special MPI variables and communication patterns

!!! abstract "Take Away"
    -  `MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag, void *recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag, MPI_Comm comm, MPI_Status * status)` This function wrap `send/recv`, but the communicator must be the same.

## 3.1 Message chain

=== ":octicons-terminal-16: Send & Recv"
    - Every sender sends message with tag `tag+1` to `rank+1` receiver
    - Every receiver gets message with tag `tag` from `rank-1` sender
    ```c
    destination = myid < ntasks - 1 ? myid + 1 : MPI_PROC_NULL;
    source = myid > 0 ? myid - 1 : MPI_PROC_NULL;
    MPI_Send(message.data(), size, MPI_INT, destination, myid+1, MPI_COMM_WORLD);
    printf("Sender: %d. Sent elements: %d. Tag: %d. Receiver: %d\n",
           myid, size, myid + 1, destination);
    MPI_Recv(receiveBuffer.data(), size, MPI_INT, source, myid, MPI_COMM_WORLD,&status);
    printf("Receiver: %d. first element %d.\n",
           myid, receiveBuffer[0]);
    ```
=== ":octicons-terminal-16: Sendrecv"
    - Use `Sendrecv` function can wrap two function together.
    ```c
    destination = myid + 1; // ignore edge cases 
    source = myid - 1;
    MPI_Sendrecv(message, size, MPI_INT, destination, myid + 1,
                receiveBuffer, size, MPI_INT, source, MPI_ANY_TAG,
                MPI_COMM_WORLD, &status);
    ```
=== ":octicons-code-review-16: Puhti Result"
    ```sh
    Sender: 0. Sent elements: 10000000. Tag: 1. Receiver: 1
    Receiver: 0. first element -1.
    Sender: 1. Sent elements: 10000000. Tag: 2. Receiver: 2
    Receiver: 1. first element 0.
    Sender: 2. Sent elements: 10000000. Tag: 3. Receiver: 3
    Receiver: 2. first element 1.
    Sender: 3. Sent elements: 10000000. Tag: 4. Receiver: -2
    Receiver: 3. first element 2.
    Time elapsed in rank  0:  0.023
    Time elapsed in rank  1:  0.023
    Time elapsed in rank  2:  0.015
    Time elapsed in rank  3:  0.008
    ```

!!! question "Investigate the timings with different numbers of MPI tasks (e.g. 2, 4, 8, 16, ...)"
    With `recv` and `send`:

    - 2 nodes -> Time elapsed in rank  0:  0.007
    - 4 nodes -> Time elapsed in rank  0:  0.023
    - 8 nodes -> Time elapsed in rank  0:  0.119
    - 16 nodes -> Time elapsed in rank  0:  0.177
    - Some of nodes always spend more time than others, time differs a lot.
  
    With `Sendrecv`:

    - 2 nodes -> TIME LIMIT?(Potential deadlock )
    - 4 nodes -> Time elapsed in rank  0:  0.042
    - 8 nodes -> Time elapsed in rank  0:  0.086
    - 16 nodes -> Time elapsed in rank  0:  0.014
    - All the node spend almost same time.

## 3.2 Parallel pi general

=== ":octicons-terminal-16: Code"

    - `istart` and `istop` is determined by `myid`
    - node 0 is responsible for receiving other's `pi`
    - other nodes sent message to node 0, tag is their own id.

    ```cpp
    int myid, istart, istop, size;
    MPI_Comm_size( MPI_COMM_WORLD , &size);
    MPI_Comm_rank(MPI_COMM_WORLD, &myid);
    int num_per_node = n/size;

    istart = myid * num_per_node + 1;
    istop = (myid  + 1)* num_per_node;

    // ...
    if(myid != 0){
        MPI_Send( &pi, 1, MPI_DOUBLE, 0 , myid, MPI_COMM_WORLD);
    }
    if(myid == 0)
    {
        for(int i = 1; i < size; i++){
        double pi_other;
        MPI_Recv( &pi_other , 1, MPI_DOUBLE , i , i , MPI_COMM_WORLD , &status);
        pi += pi_other;
        }
    }    
    ```
=== ":octicons-code-review-16: Puhti Result(4 nodes)"
    ```
    Node 1 compute result: 0.874676
    Node 0 compute result: 0.979915
    Computing approximation to pi with N=840
    Each one is responsible for 210 elements.
    Approximate pi=3.1415927716925900 (exact pi=3.14159265)
    Node 2 compute result: 0.719414
    Node 3 compute result: 0.567588
    ```
=== ":octicons-code-review-16: Puhti Result(8 nodes)"
    ```
    Node 0 compute result: 0.497420
    Computing approximation to pi with N=840
    Each one is responsible for 105 elements.
    Approximate pi=3.1415927716925909 (exact pi=3.14159265)
    Node 3 compute result: 0.419508
    Node 4 compute result: 0.379807
    Node 2 compute result: 0.455168
    Node 5 compute result: 0.339607
    Node 6 compute result: 0.301316
    Node 1 compute result: 0.482495
    Node 7 compute result: 0.266273
    ```

!!! question "Run the code with different number of processes, do you get exactly the same result?"
    - No, same reason as before. Loss of accuracy because of floating number division.

!!! question "With MPI_ANY_SOURCE, do you get exactly the same result when run multiple times?"
    - same: This is a deterministic program, I don't see any random parts.
    - 1st: 3.1415927716925895
    - 2nd: 3.1415927716925895
    - 3rd: 3.1415927716925895

!!! question "Make a version that works with arbitrary number of processes."
    - Simple version: give all the job to the last node `if(myid == size -1){ istop = n; }`
    - Fair version: give one to each node. The index is arranged as
    ```cpp
      istart =  myid * num_per_node + 1 +  ((myid < remainder)? myid: remainder);
      istop = (myid  + 1)* num_per_node + ((myid < remainder)? myid + 1: remainder);
    ```

## 3.3 Broadcast and scatter

ðŸ”˜ **Broadcast**:

=== ":octicons-terminal-16: Code"
    - In my answer the local `recvbuf` in node 0 also receive by `MPI_Recv`. But it should be transferred locally(potentially deadlock)?

    ```cpp
    if(0 == myid){
        for(int i = 0; i < ntasks; i ++ ){
        MPI_Send( sendbuf.data() , size, MPI_INT , i , i , MPI_COMM_WORLD);
        }
    }
    MPI_Recv(recvbuf.data(), size, MPI_INT, 0, myid, MPI_COMM_WORLD, &status);
    ```
=== ":octicons-code-review-16: Puhti Result"

    ```sh
    Task  0:  0  1  2  3  4  5  6  7  8  9 10 11
    Task  1: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
    Task  2: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
    Task  3: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1

    Task  0:  0  1  2  3  4  5  6  7  8  9 10 11
    Task  1:  0  1  2  3  4  5  6  7  8  9 10 11
    Task  2:  0  1  2  3  4  5  6  7  8  9 10 11
    Task  3:  0  1  2  3  4  5  6  7  8  9 10 11
    ```

ðŸ”˜ **Scatter** 

=== ":octicons-terminal-16: Code"
    - In my answer I put the received data in its original place.

    ```cpp
    int block_size = size / ntasks;
    if(0 == myid){
        for(int i = 0; i < ntasks; i ++ )
        MPI_Send( sendbuf.data() + block_size*i, block_size, MPI_INT , i , i , MPI_COMM_WORLD);
    }
    MPI_Recv(recvbuf.data() + block_size* myid, block_size, MPI_INT, 0, myid, MPI_COMM_WORLD, &status);
    ```
=== ":octicons-code-review-16: Puhti Result"

    ```sh
    Task  0:  0  1  2  3  4  5  6  7  8  9 10 11
    Task  1: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
    Task  2: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1
    Task  3: -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1

    Task  0:  0  1  2 -1 -1 -1 -1 -1 -1 -1 -1 -1
    Task  1: -1 -1 -1  3  4  5 -1 -1 -1 -1 -1 -1
    Task  2: -1 -1 -1 -1 -1 -1  6  7  8 -1 -1 -1
    Task  3: -1 -1 -1 -1 -1 -1 -1 -1 -1  9 10 11
    ```