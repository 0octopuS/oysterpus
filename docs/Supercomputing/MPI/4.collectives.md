# 4. Collectives

!!! abstract "Take Away"
    - `MPI_Bcast( ..., int root , ...)` broadcast data from `root` to any other node.
    - `MPI_Scatter()`: Scatters data from one member across all members of a group.(reverse of `MPI_Gather`)
    - `MPI_Gatherv(... , const MPI_Count recvcounts[] , const MPI_Aint displs[] ,...)` Gathers variable data from all members of a group to one member.
        - `recvcounts[j]` is the number of elements that is received from process `j`.
        - -The data that is received from process `j` is placed into the `recvbuf` of the `root` process offset `displs[j]` elements from the `sendbuf` pointer.
    - `MPI_Alltoall()` Gathers data from and scatters data to all members of a group. The `j` block that is sent from process `i` is received by process `j` and is placed in the `i` block of the `recvbuf`.

## 4.1 Broadcast

=== ":octicons-terminal-16: Code"
    - use `sendbuf` as target buffer.
    ```cpp
    MPI_Bcast( sendbuf.data() , size, MPI_INT , 0, MPI_COMM_WORLD);
    ```
=== ":octicons-code-review-16: Puhti Result"
    ```sh
    Task  0:  0  1  2  3  4  5  6  7
    Task  1:  8  9 10 11 12 13 14 15
    Task  2: 16 17 18 19 20 21 22 23
    Task  3: 24 25 26 27 28 29 30 31

    Task  0:  0  1  2  3  4  5  6  7
    Task  1:  0  1  2  3  4  5  6  7
    Task  2:  0  1  2  3  4  5  6  7
    Task  3:  0  1  2  3  4  5  6  7
    ```

## 4.2 Scatter

=== ":octicons-terminal-16: Code"
    - use `scatter` function.
    ```cpp
    int blocksize = size / ntasks;
     MPI_Scatter( sendbuf.data() , blocksize , MPI_INT , recvbuf.data(), blocksize, MPI_INT , 0 , MPI_COMM_WORLD);
    ```
=== ":octicons-code-review-16: Puhti Result"

    ````sh
    Task  0:  0  1  2  3  4  5  6  7
    Task  1:  8  9 10 11 12 13 14 15
    Task  2: 16 17 18 19 20 21 22 23
    Task  3: 24 25 26 27 28 29 30 31

    Task  0:  0  1 -1 -1 -1 -1 -1 -1
    Task  1:  2  3 -1 -1 -1 -1 -1 -1
    Task  2:  4  5 -1 -1 -1 -1 -1 -1
    Task  3:  6  7 -1 -1 -1 -1 -1 -1
    ````

## 4.3 Gather

=== ":octicons-terminal-16: Code"
    - Use `Gatherv` function to specify how many elements received in other processor and where to put it.
    ```cpp
    int offsets[NTASKS] = { 0, 1, 2, 4 };
    int counts[NTASKS] = { 1, 1, 2, 4 };
    MPI_Gatherv(sendbuf.data(), counts[rank], MPI_INT, recvbuf.data(), counts,
                offsets, MPI_INT, 1, MPI_COMM_WORLD);
    ```
    - `offset` can also be
    ```cpp
    offset[0] = 0; 
    for(int i = 1; i < NTASKS; i++) 
        offsets[i] = offsets[i-1] + counts[i-1]; 
    ```

=== ":octicons-code-review-16: Puhti Result"
    ```sh
    Task  0:  0  1  2  3  4  5  6  7
    Task  1:  8  9 10 11 12 13 14 15
    Task  2: 16 17 18 19 20 21 22 23
    Task  3: 24 25 26 27 28 29 30 31

    Task  0: -1 -1 -1 -1 -1 -1 -1 -1
    Task  1:  0  8 16 17 24 25 26 27
    Task  2: -1 -1 -1 -1 -1 -1 -1 -1
    Task  3: -1 -1 -1 -1 -1 -1 -1 -1
    ```

## 4.4 Alltoall

=== ":octicons-terminal-16: Code"
    - `Alltoall` will put the data send from `j` processor to `j` block in `recvbuf`. No need to manually arrange index.
    ```cpp
    int blocksize = size / ntasks;
    MPI_Alltoall(sendbuf.data() , blocksize , MPI_INT , recvbuf.data(), blocksize ,MPI_INT, MPI_COMM_WORLD);
    ```
=== ":octicons-code-review-16: Puhti Result"
    ```sh
    Task  0:  0  1  2  3  4  5  6  7
    Task  1:  8  9 10 11 12 13 14 15
    Task  2: 16 17 18 19 20 21 22 23
    Task  3: 24 25 26 27 28 29 30 31

    Task  0:  0  1  8  9 16 17 24 25
    Task  1:  2  3 10 11 18 19 26 27
    Task  2:  4  5 12 13 20 21 28 29
    Task  3:  6  7 14 15 22 23 30 31
    ```