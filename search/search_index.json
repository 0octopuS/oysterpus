{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\/\\[\\]()\"`]+|\\.?(?!\\/\\d)|&[lg]t;|(?!\\/b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"index.html#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"index.html#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html","title":"Google Cloud Fundamentals: Core Infrastructure","text":"<p>https://googlecloudcheatsheet.withgoogle.com/</p>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#1-components","title":"1. Components","text":"<p>Contents</p> <ul> <li> VM and Network in GCP</li> <li> Storage in GCP</li> <li> Container</li> <li> Application</li> </ul>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#2-virtual-machines-and-network","title":"2. Virtual Machines and Network","text":""},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#3-storage","title":"3. Storage","text":"<p>Take away</p> <p>Five storage products:</p> <ul> <li>Cloud Storage: general object</li> <li>Cloud SQL:  relational databases, including MySQL, PostgreSQL, and SQL Server</li> <li>Cloud Spanner: relational database service that scales horizontally</li> <li>Firestore: flexible, horizontally scalable, NoSQL cloud database for mobile, web, and server development. </li> <li>Cloud BigTable: NoSQL Big data database service.</li> </ul> <p></p>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#cloud-storage","title":"Cloud Storage","text":"<p>\ud83d\udd18 Use case: BLOB for online contents(video, images...)</p> <p>\ud83d\udd18 Object storage: is a computer data storage architecture that manages data as \u201cObjectives\u201d and not as a file and folder hierarchy (file storage), or as chunks of a disk (block storage).</p> <p>\ud83d\udd18 Components: Cloud Storage files are organized into buckets. Choose ideal location for a bucket is where latency is minimized.</p> <p>\ud83d\udd18 Immutability of Objectives: The storage Objectives offered by Cloud Storage are immutable. To update, either overwrite, or using versioning to keep track of previous file.</p> <p>\ud83d\udd18 ACL: IAM is sufficient in most cases, roles are inherited from project to bucket to object. For finer control, ACLs can be created with [Scope, Permission].</p> <p>\ud83d\udd18 Tiers of Storage</p> Type Use(read/modify) Frequency Standard Storage Hot data Nearline Storage 30 days Coldline Storage 90 days Archive Storage 1 year"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#cloud-sql","title":"Cloud SQL","text":"<p>Cloud SQL includes MySQL, PostgreSQL, and SQL Server.</p> <p>\ud83d\udd18 Mundane but necessary tasks: GCP manage applying patches and updates, managing backups, and configuring replications.</p> <p>\ud83d\udd18 Location and accessibility: can be configure to near Compute Engines. Could SQ can be accessed by other google Cloud services and even external services.</p>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#cloud-spanner","title":"Cloud Spanner","text":"<p>\ud83d\udd18 Use cases</p> <ul> <li>A SQL relational database management system with joins and secondary indexes.</li> <li>Built-in high availability</li> <li>Strong global consistency</li> <li>High numbers of input and output operations per second.</li> </ul>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#firestore","title":"Firestore","text":"<p>Data is stored in documents and then organized into collections.</p> <p>\ud83d\udd18 Compare to mongodb</p> <ul> <li>MongoDB accepts query language to retrieve data, but Firestore has its own method and API calls. </li> <li>Firestore has advantage in data synchronization. The capability to update and listen to changes in the database in real-time is pretty neat when working with web/mobile applications, giving all the users the feeling they are updated all the time.</li> </ul>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#cloud-bigtable","title":"Cloud BigTable","text":"<p>Bigtable is designed to handle massive workloads at consistent low latency and high throughput.</p>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#storage-compare","title":"Storage Compare","text":""},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#4-container","title":"4. Container","text":"<p>Take Away</p> <ul> <li>Container is PaaS. </li> <li>Kubernetes is for managing and scaling containerized applications.</li> </ul>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#container","title":"Container","text":"<p>\ud83d\udd18 A container, is an invisible box around your code and its dependencies with limited access to its own partition of the file system and hardware.</p>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#kubernetes","title":"Kubernetes","text":"<p>\ud83d\udd18 Kubernetes is an open source platform for managing containerized workloads and services.  Kubernetes is a set of APIs that you can use to deploy containers on a set of nodes called a cluster.</p> <p>\ud83d\udd18 Terminal in Kubernetes</p> <ul> <li>A node represents a computing instance like a machine.</li> <li>A pod is the smallest unit in Kubernetes that you can create or deploy. It represents a running process on your cluster as either a component of your application or an entire app. Generally 1 container for 1 pod. The pod provides a unique network IP and set of ports for your containers.</li> <li>A deployment represents a group of replicas of the same pod and keeps your pods running even when the nodes they run on fail.</li> <li>A service is an abstraction which defines a logical set of pods and a policy by which to access them.</li> <li>A service group is a set of pods and provides a stable endpoint or fixed IP address for them.</li> </ul> <p>\ud83d\udd18 Structure of Kubernetes</p>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#gkegoogle-kubernetes-engine","title":"GKE(Google Kubernetes Engine)","text":"<p>\ud83d\udd18 GKE is a Google hosted managed Kubernetes service in the cloud.</p> <p>\ud83d\udd18 Some features GCP provides</p> <ul> <li>Load balancing for compute engine instances</li> <li>node pools to designate subsets of nodes within a cluster</li> <li>Automatic scaling of your cluster's node instance count</li> <li>Automatic upgrades for your cluster's node software</li> <li>Node auto-repair to maintain node health and availability</li> <li>Logging and monitoring with Google Cloud's operation suite </li> </ul>"},{"location":"DevOps/GCPDevOps/1.core_infrastructure.html#5-application","title":"5. Application","text":"<p>Take Away</p> <ul> <li>Cloud Run and Cloud Functions are serverless.</li> <li>Cloud Run is for application.</li> <li>Cloud Function is for a single purpose function.</li> </ul> <p>\ud83d\udd18 Cloud Run is a managed compute platform that runs stateless containers via web requests or pub/sub events.</p> <p>\ud83d\udd18 Cloud Run workflow: write code -&gt; build package -&gt; deploy. Cloud Run then starts your container on demand and dynamically scaling with requests amount.</p> <p>\ud83d\udd18 Cloud Function require no management of server or a runtime environment.</p>"},{"location":"DevOps/GCPDevOps/2.terraform.html","title":"Getting Started with Terraform for Google Cloud","text":"<p>Terraform is an open source, Infrastructure as Code tool by HashiCorp, for provisioning resources \u2013 including Google Cloud resources \u2013 with declarative configuration files.</p>"},{"location":"DevOps/GCPDevOps/2.terraform.html#1-introduction-to-terraform-for-google-cloud","title":"1. Introduction to Terraform for Google Cloud","text":"<p>\ud83d\udd18 Infrastructure as code(IaC): declare the desired end state of the infrastructure \u2013IaC handles management and provisioning.</p> <p>\ud83d\udd18 Terraform: Terraform allows infrastructure to be expressed as code in a simple, human-readable language called HashiCorp Configuration Language, or HCL. Terraform reads configuration files and provides an execution plan of changes, which can be reviewed, applied, and provisioned.</p> <p>\ud83d\udd18 IaC configuration workflow</p>"},{"location":"DevOps/GCPDevOps/2.terraform.html#2-terms-and-concepts","title":"2. Terms and Concepts","text":""},{"location":"DevOps/GCPDevOps/2.terraform.html#terraform-configuration-and-hcl","title":"Terraform configuration and HCL","text":"<ul> <li>A Terraform configuration is a complete document in the Terraform language that tells Terraform how to manage a given collection of infrastructure.</li> <li>Store in .tf file.</li> <li>Modules: A root module(root configuration file), And an optional tree for child modules.</li> <li>HashiCorp Configuration Language(HCL): a JSON-based variant. HCL includes a limited set of primitives such as <code>variables</code>, <code>resources</code>, <code>outputs</code>, and <code>modules</code>.</li> </ul> <pre><code>&lt;Block Type&gt; &lt;Block Label&gt; &lt;Block Label&gt; {\n    &lt;Identifier&gt; = &lt;Expression&gt;\n}\n</code></pre>"},{"location":"DevOps/GCPDevOps/2.terraform.html#author-phase-terms-and-concepts","title":"Author phase terms and concepts","text":"<ul> <li>Resources are code blocks that define the infrastructure components. <code>resource \"google_compute_instance\" \"vm_name\" {...}</code></li> <li> <p>Providers implement every resource type</p> <pre><code>terraform{\n   required_providers{ \n       google={ \n           source = \"hashicorp/google\",\n       }\n   }\n}\n\nprovider \"google { project = &lt;PROJECT&gt;, region=&lt;PROJECT_REGION&gt;}\n</code></pre> </li> <li> <p>Variables are used to parameterize your configuration.  define a resource attribute at run time or centrally in a file with a <code>.tfvar</code> file.</p> </li> <li> <p>Output values are a way to expose some of that configuration information.</p> <pre><code>output \"bucket_URL\"{\n    value = google_storage_bucket.mybucket.URL\n}\n</code></pre> </li> <li> <p>Terraform saves the state of resources that it manages in a state file(terraform.tfstate). Do not modify or touch this file; it\u2019s created and updated automatically.</p> </li> <li>a Terraform module is a set of Terraform configuration files in a single directory.</li> </ul>"},{"location":"DevOps/GCPDevOps/2.terraform.html#terraform-commands","title":"Terraform commands","text":"<pre><code>terraform init # a hidden directory called terraform is created inside the current working directory. It will initialize provider plugins.\nterraform plan # creates an execution plan detailing all the resources that will be created, modified, or destroyed upon executing terraform apply.\nterraform apply # executes the actions proposed in a Terraform plan, creates the resources, and establishes the dependencies.\nterraform destroy # Terraform determines the order in which things must be destroyed.\nterraform validate # validate runs checks that verify whether a configuration is syntactically valid and internally consistent\nterraform fmt #applies all formatting rules and recommended styles to assist with readability and consistency.\n</code></pre>"},{"location":"DevOps/GCPDevOps/2.terraform.html#terraform-validator","title":"Terraform validator","text":"<p>terraform validate vs terraform validator</p> <ul> <li><code>terraform validate</code> command checks grammar.</li> <li>The Terraform Validator is used to ensure that the configuration adheres to the set of constraints.   </li> </ul> <p>The Terraform validator is a tool for enforcing policy compliance as part of an infrastructure CI/CD pipeline.</p> <pre><code>gcloud beta terraform vet\n</code></pre> <p>The tool retrieves project data with Google Cloud APIs, so you can accurately validate your plan.</p>"},{"location":"DevOps/GCPDevOps/2.terraform.html#writing-iac-for-google-cloud","title":"Writing IaC for Google Cloud","text":""},{"location":"DevOps/GCPDevOps/lab1.foundational_Infrastructure_tasks.html","title":"LAB: Perform Foundational Infrastructure Tasks in Google Cloud","text":""},{"location":"DevOps/GCPDevOps/lab1.foundational_Infrastructure_tasks.html#1-cloud-functions","title":"1. Cloud Functions","text":"<p>Objects</p> <ul> <li>Create a cloud function</li> <li>Deploy and test the function</li> <li>View logs</li> </ul>"},{"location":"DevOps/GCPDevOps/lab1.foundational_Infrastructure_tasks.html#cloud-function-with-consoles","title":"Cloud function with consoles","text":"<p>\ud83d\udd18 Step1: console -&gt; Cloud Function -&gt; create function -&gt; edit setting \ud83d\udd18 Step2: write the function -&gt; deploy \ud83d\udd18 Step3: testing -&gt; write test http request -&gt; send tests \ud83d\udd18 Step4: view logging -&gt; the log history that displays in Query results</p>"},{"location":"DevOps/GCPDevOps/lab1.foundational_Infrastructure_tasks.html#cloud-function-with-command-line","title":"Cloud function with command line","text":"<p>\ud83d\udd18 Step1: create a function. This is just normal linux operation.</p> <pre><code>gcloud config set compute/region us-central1  # set the default region\nmkdir gcf_hello_world\ncd gcf_hello_world\nnano index.js\n</code></pre> <p>\ud83d\udd18 Step2: create a cloud bucket</p> <p><code>mb</code> stands for \"make bucket\". The second varibale is <code>gs://[bucket_name]</code>, here we use project id as bucket name.</p> <pre><code>gsutil mb -p $GOOGLE_CLOUD_PROJECT gs://$GOOGLE_CLOUD_PROJECT\n</code></pre> <p>\ud83d\udd18 Step3: deploy function</p> <ul> <li>use <code>gcloud functions deploy</code> to deploy function with given bucket, trigger and runtime setting.</li> <li>use <code>gcloud functions describe</code> to vie the state of function</li> </ul> <pre><code>gcloud functions deploy helloWorld \\\n--stage-bucket $GOOGLE_CLOUD_PROJECT \\\n--trigger-topic hello_world \\\n--runtime nodejs20\n\ngcloud functions describe helloWorld\n</code></pre> <p>\ud83d\udd18 Step4: test function</p> <p>use <code>gcloud functions call</code> to test function.</p> <pre><code>DATA=$(printf 'Hello World!'|base64) &amp;&amp; gcloud functions call helloWorld --data '{\"data\":\"'$DATA'\"}'\n</code></pre> <p>\ud83d\udd18 Step5: view logs</p> <p>use <code>gcloud functions logs read</code> to read the logs of a function.</p> <pre><code>gcloud functions logs read helloWorld\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab1.foundational_Infrastructure_tasks.html#2-pubsub","title":"2. Pub/Sub","text":"<p>Objects</p> <ul> <li>Set up a topic to hold data.</li> <li>Subscribe to a topic to access the data.</li> <li>Publish and then consume messages with a pull subscriber.</li> </ul>"},{"location":"DevOps/GCPDevOps/lab1.foundational_Infrastructure_tasks.html#pubsub-from-console","title":"Pub/Sub from console","text":"<p>\ud83d\udd18 Step 1 setting up pub/sub:  Go to pub/sub -&gt; create topic</p> <p>\ud83d\udd18 Step 2 add a subscription: Go to pub/sub &gt; Topics -&gt; create subscription</p> <p>\ud83d\udd18 Step 3 public a message to topic: Go to pub/sub &gt; Topics</p> <p>\ud83d\udd18 Step 4: view the message: Go to subscription -&gt; message -&gt; bulic</p>"},{"location":"DevOps/GCPDevOps/lab1.foundational_Infrastructure_tasks.html#pubsub-from-command-line","title":"Pub/Sub from command line","text":"<p>\ud83d\udd18 Step 1: setting up pub/sub</p> <pre><code>gcloud pubsub topics create myTopic\ngcloud pubsub topics create Test1\ngcloud pubsub topics create Test2\ngcloud pubsub topics list\n</code></pre> <p>\ud83d\udd18 Step 2: add a subscription</p> <pre><code>gcloud pubsub subscriptions create --topic myTopic mySubscription\ngcloud pubsub subscriptions delete myTopic\n\ngcloud pubsub topics list-subscriptions myTopic\n</code></pre> <p>\ud83d\udd18 Step 3: publish and pull a message to/from topic</p> <p>To publish a message to a topic, use <code>gcloud pubsub topics publish [topic] -- message [msg]</code>.</p> <pre><code>gcloud pubsub topics publish myTopic --message \"Hello\"\ngcloud pubsub topics publish myTopic --message \"Publisher's name is &lt;YOUR NAME&gt;\"\n</code></pre> <p>To pull a single msg, use the flowing command. Once an individual message has been outputted from a particular subscription-based pull command, you cannot access that message again with the pull command.</p> <pre><code>gcloud pubsub subscriptions pull mySubscription --auto-ack\n</code></pre> <p>\ud83d\udd18 Step 4: pulling all messages</p> <p>To pull multiple messages, use <code>pull --limit [number]</code>.</p> <pre><code>gcloud pubsub subscriptions pull mySubscription --auto-ack --limit=3\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab1.foundational_Infrastructure_tasks.html#pubsub-from-apipython","title":"Pub/Sub from API(Python)","text":"<p>\ud83d\udd18 Step 1:  Create a virtual environment</p> <p>Python virtual environments are used to isolate package installation from the system.</p> <pre><code>sudo apt-get install -y virtualenv\npython3 -m venv venv\nsource venv/bin/activate\n</code></pre> <p>\ud83d\udd18 Step 2: Install the client library</p> <pre><code>pip install --upgrade google-cloud-pubsub\ngit clone https://github.com/googleapis/python-pubsub.git\ncd python-pubsub/samples/snippets\n</code></pre> <p>\ud83d\udd18 Step 3: Create a topic</p> <pre><code>echo $GOOGLE_CLOUD_PROJECT\ncat publisher.py\npython publisher.py -h\npython publisher.py $GOOGLE_CLOUD_PROJECT create MyTopic\n</code></pre> <p>Topic is created using <code>pubsub_v1.PublisherClient().create_topic(topic_name)</code>,  <code>topic_name</code> should follow the format below.</p> <pre><code>import os\nfrom google.cloud import pubsub_v1\n\npublisher = pubsub_v1.PublisherClient()\ntopic_name = 'projects/{project_id}/topics/{topic}'.format(\n    project_id=os.getenv('GOOGLE_CLOUD_PROJECT'),\n    topic='MY_TOPIC_NAME',  # Set this to something appropriate.\n)\npublisher.create_topic(name=topic_name)\n</code></pre> <p>\ud83d\udd18 Step 4: Create a subscription</p> <pre><code>python subscriber.py $GOOGLE_CLOUD_PROJECT create MyTopic MySub\npython subscriber.py $GOOGLE_CLOUD_PROJECT list-in-project\n</code></pre> <p>Subscription is created/subscribe by these api calls.</p> <pre><code>with pubsub_v1.SubscriberClient() as subscriber:\n  subscriber.create_subscription(\n      name=subscription_name, topic=topic_name)\n  future = subscriber.subscribe(subscription_name, callback)\n</code></pre> <p>\ud83d\udd18 Step 5: Publish messages</p> <pre><code>gcloud pubsub topics publish MyTopic --message \"Hello\"\ngcloud pubsub topics publish MyTopic --message \"Publisher's name is &lt;YOUR NAME&gt;\"\ngcloud pubsub topics publish MyTopic --message \"Publisher likes to eat &lt;FOOD&gt;\"\n</code></pre> <p>Message is published using <code>pubsub_v1.PublisherClient().publish(topic_name, msg, spam)</code>.</p> <pre><code>future = publisher.publish(topic_name, b'My first message!', spam='eggs')\nfuture.result()\n</code></pre> <p>\ud83d\udd18 Step 6. View messages</p> <pre><code>python subscriber.py $GOOGLE_CLOUD_PROJECT receive MySub\n</code></pre> <p>The message can be viewed using the result of <code>subscriber.subscribe().result()</code>.</p> <pre><code> streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)\n    print(f\"Listening for messages on {subscription_path}..\\n\")\n\n    # Wrap subscriber in a 'with' block to automatically call close() when done.\n    with subscriber:\n        try:\n            # When `timeout` is not set, result() will block indefinitely,\n            # unless an exception is encountered first.\n            streaming_pull_future.result(timeout=timeout)\n        except TimeoutError:\n            streaming_pull_future.cancel()  # Trigger the shutdown.\n            streaming_pull_future.result()  # Block until the shutdown is complete.\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html","title":"LAB: Automating Infrastructure on Google Cloud with Terraform","text":""},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#1-terraform-fundamentals","title":"1. Terraform Fundamentals","text":"<p>Terraform is a tool for building, changing, and versioning infrastructure safely and efficiently. Key features: IaC, Execution plans, Resource graph, Change automation</p> <p>Objectives</p> <ul> <li>Get started with Terraform in Google Cloud.</li> <li>Install Terraform from installation binaries.</li> <li>Create a VM instance infrastructure using Terraform.</li> </ul>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#lab-step","title":"Lab Step","text":"<p>\ud83d\udd18 Step 1: create <code>instance.tf</code> file as following.</p> <p>It specifies creating a compute instance, naming \"terraform. \\ And for this instance, it specified its project, name, machine_type and other setting, similar to the setting in Google Cloud Console.</p> <pre><code>resource \"google_compute_instance\" \"terraform\" {\n  project      = \"qwiklabs-gcp-01-ee27324076c2\"\n  name         = \"terraform\"\n  machine_type = \"e2-medium\"\n  zone         = \"us-central1-f\"\n\n  boot_disk {\n    initialize_params {\n      image = \"debian-cloud/debian-11\"\n    }\n  }\n\n  network_interface {\n    network = \"default\"\n    access_config {\n    }\n  }\n}\n</code></pre> <p>\ud83d\udd18 Step 2: use terraform to plan</p> <p>First initialize, then plan using the .tf file.</p> <pre><code>terraform init\nterraform plan\n</code></pre> <p>The plan result is following:</p> <pre><code>terraform plan\n\nTerraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:\n  + create\n\nTerraform will perform the following actions:\n\n  # google_compute_instance.terraform will be created\n  + resource \"google_compute_instance\" \"terraform\" {\n      + can_ip_forward       = false\n      + cpu_platform         = (known after apply)\n      + current_status       = (known after apply)\n      + deletion_protection  = false\n      + effective_labels     = (known after apply)\n      + guest_accelerator    = (known after apply)\n      + id                   = (known after apply)\n      + instance_id          = (known after apply)\n      + label_fingerprint    = (known after apply)\n      + machine_type         = \"e2-medium\"\n      + metadata_fingerprint = (known after apply)\n      + min_cpu_platform     = (known after apply)\n      + name                 = \"terraform\"\n      + project              = \"qwiklabs-gcp-01-ee27324076c2\"\n      + self_link            = (known after apply)\n      + tags_fingerprint     = (known after apply)\n      + terraform_labels     = (known after apply)\n      + zone                 = \"us-central1-f\"\n\n      + boot_disk {\n          + auto_delete                = true\n          + device_name                = (known after apply)\n          + disk_encryption_key_sha256 = (known after apply)\n          + kms_key_self_link          = (known after apply)\n          + mode                       = \"READ_WRITE\"\n          + source                     = (known after apply)\n\n          + initialize_params {\n              + image                  = \"debian-cloud/debian-11\"\n              + labels                 = (known after apply)\n              + provisioned_iops       = (known after apply)\n              + provisioned_throughput = (known after apply)\n              + size                   = (known after apply)\n              + type                   = (known after apply)\n            }\n        }\n\n      + network_interface {\n          + internal_ipv6_prefix_length = (known after apply)\n          + ipv6_access_type            = (known after apply)\n          + ipv6_address                = (known after apply)\n          + name                        = (known after apply)\n          + network                     = \"default\"\n          + network_ip                  = (known after apply)\n          + stack_type                  = (known after apply)\n          + subnetwork                  = (known after apply)\n          + subnetwork_project          = (known after apply)\n\n          + access_config {\n              + nat_ip       = (known after apply)\n              + network_tier = (known after apply)\n            }\n        }\n    }\n\nPlan: 1 to add, 0 to change, 0 to destroy.\n</code></pre> <p>\ud83d\udd18 Step 3: apply the change</p> <p>If the plan was created successfully, Terraform will now pause and wait for approval before proceeding. Use <code>terraform apply</code> to apply the .tf file. Inspect the state by <code>terraform show</code>.</p>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#2-infrastructure-as-code-with-terraform","title":"2. Infrastructure as Code with Terraform","text":"<p>Objectives</p> <ul> <li>Build, change, and destroy infrastructure with Terraform</li> <li>Create Resource Dependencies with Terraform</li> <li>Provision infrastructure with Terraform</li> </ul>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#lab-steps","title":"Lab Steps","text":"<p>\ud83d\udd18 Step 1: Build infrastructure</p> <pre><code>touch main.tf\n</code></pre> <p>\ud83d\udd18 Step : \ud83d\udd18 Step : \ud83d\udd18 Step : \ud83d\udd18 Step :</p>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#3-interact-with-terraform-modules","title":"3. Interact with Terraform Modules","text":"<p>A Terraform module is a set of Terraform configuration files in a single directory. Even a simple configuration consisting of a single directory with one or more .tf files is a module.</p> <p>Objectives</p> <ul> <li>Use a module from the Registry</li> <li>Build a module</li> </ul> <p>The structure usually like</p> <pre><code>\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 main.tf\n\u251c\u2500\u2500 variables.tf\n\u251c\u2500\u2500 outputs.tf\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#using-module-from-registry","title":"Using module from registry","text":"<p>\ud83d\udd18 <code>main.tf</code> contains a <code>module</code>, specified the source from terraform registry.</p> <p><code>module</code> \"test-vpc-module\" defines a Virtual Private Cloud (VPC), which will provide networking services for the rest of your infrastructure.</p> <pre><code>module \"test-vpc-module\" {\n  source       = \"terraform-google-modules/network/google\"\n  version      = \"~&gt; 6.0\"\n  project_id   = var.project_id # Replace this with your project ID\n  network_name = \"my-custom-mode-network\"\n  mtu          = 1460\n\n  subnets = [{subnet_name   = \"subnet-01\"...}, {...}, ...]\n\n}\n</code></pre> <p>\ud83d\udd18 <code>variable.tf</code> contains variables with their description, default value and so on.</p> <p>These variable can be used in other file by <code>var.variable_name</code>.</p> <pre><code>variable \"project_id\" {\n  description = \"The project ID to host the network in\"\n  default     = \"FILL IN YOUR PROJECT ID HERE\"\n}\n</code></pre> <p>\ud83d\udd18 <code>output.tf</code> contains the output value by the module, which can be accessed by referring to <code>module.&lt;MODULE NAME&gt;.&lt;OUTPUT NAME&gt;</code>.</p> <pre><code>output \"network_name\" {\n  value       = module.test-vpc-module.network_name\n  description = \"The name of the VPC being created\"\n}\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#define-own-module","title":"Define own module","text":"<p>Define the following file structure:</p> <pre><code>main.tf\noutputs.tf\nvariables.tf\nmodules/\n\u2514\u2500\u2500 gcs-static-website-bucket\n    \u251c\u2500\u2500 LICENSE\n    \u251c\u2500\u2500 README.md\n    \u251c\u2500\u2500 website.tf\n    \u251c\u2500\u2500 outputs.tf\n    \u2514\u2500\u2500 variables.tf\n</code></pre> <p>\ud83d\udd18 <code>modules/website.tf</code> defines the <code>resource</code></p> <pre><code>resource \"google_storage_bucket\" \"bucket\" {\n}\n</code></pre> <p>\ud83d\udd18 <code>modules/variable.tf</code> and <code>modules/output.tf</code> are similar to previous file. There is no <code>main.tf</code> file in the module.</p> <p>\ud83d\udd18 <code>main.tf</code> can use module by referring it.</p> <pre><code>module \"gcs-static-website-bucket\" {\nsource = \"./modules/gcs-static-website-bucket\"\n...\n}\n</code></pre> <p>\ud83d\udd18 <code>output.tf</code> can access to the output of the module by</p> <pre><code>output \"bucket-name\" {\n  description = \"Bucket names.\"\n  value       = \"module.gcs-static-website-bucket.bucket\"\n}\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#4-managing-terraform-state","title":"4. Managing Terraform State","text":"<p>State is a necessary requirement for Terraform to function. </p> <p>Objectives</p> <ul> <li>Create a local backend.</li> <li>Create a Cloud Storage backend.</li> <li>Refresh your Terraform state.</li> <li>Import a Terraform configuration.</li> <li>Manage the imported configuration with Terraform.</li> </ul> <p>Why states?</p> <ul> <li>tracking the mappings between resources and remote objects(one remote object, one resource instance)</li> <li>track metadata such as resource dependencies(delete order)</li> <li>perform improvement, terraform stores a cache of the attribute values for all resources in the state</li> <li>sync for team use</li> <li>state locking to prevent file corrupt</li> </ul>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#working-with-backends","title":"Working with backends","text":"<p>A backend in Terraform determines how state is loaded and how an operation such as apply is executed.</p>"},{"location":"DevOps/GCPDevOps/lab2.automating_infrastructure_on_GC_terraform.html#import-terraform-configuration","title":"Import Terraform configuration","text":""},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html","title":"LAB: Implement DevOps in Google Cloud","text":""},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#1-google-cloud-source-repositories-setup","title":"1. Google Cloud Source Repositories Setup","text":"<p>Google Cloud Source Repositories provides Git version control to support collaborative development of any application or service.</p> <p>Objectives</p> <ul> <li>create a local Git repository</li> <li>add a Google Source Repository as a remote</li> </ul> <ul> <li><code>gcloud source repos create</code> creates new repository</li> <li><code>gcloud source repos clone</code> command add Google Cloud Source repository as remote origin.</li> </ul> <pre><code>gcloud source repos create REPO_DEMO\ngcloud source repos clone REPO_DEMO\n\n# normal git operations until committed a file\ngit push origin master\n\ngcloud source repos list\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#2-managing-deployments-using-kubernetes-engine","title":"2. Managing Deployments Using Kubernetes Engine","text":"<p>Objectives</p> <ul> <li>Practice with kubectl tool</li> <li>Create deployment yaml files</li> <li>Launch, update, and scale deployments</li> <li>Practice with updating deployments and deployment styles</li> </ul>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#create-deployment","title":"Create deployment","text":"<p>\ud83d\udd18 Step 1: <code>kubectl explain</code> command can tell the detail of a deployment object.</p> <pre><code>kubectl explain deployment --recursive\nkubectl explain deployment.metadata.name\n</code></pre> <p>\ud83d\udd18 Step 2: setting deployment configuration</p> <p>Update the <code>deployments/auth.yaml</code> configuration file, setting the <code>auth</code> container to use image <code>\"kelseyhightower/auth:1.0.0\"</code>.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: auth\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: auth\n  template:\n    metadata:\n      labels:\n        app: auth\n        track: stable\n    spec:\n      containers:\n        - name: auth\n          image: \"kelseyhightower/auth:1.0.0\"\n          ports:\n            - name: http\n              containerPort: 80\n            - name: health\n              containerPort: 81\n...\n</code></pre> <p>\ud83d\udd18 Step 3: create deployment object and inspect</p> <p>Use <code>kubectl create -f deployments/auth.yaml</code> to create deployment by the configuration file. After successfully created, it can be inspected by <code>kubectl get deployments</code>.</p> <p>A <code>ReplicaSet</code> will be created for the deployment, which can be seen by <code>kubectl get replicasets</code>.</p> <p>\ud83d\udd18 Step 3: create service object</p> <pre><code>kubectl create -f services/auth.yaml\n\n# create another deployment/service set.\nkubectl create -f deployments/hello.yaml\nkubectl create -f services/hello.yaml\n\n# add secret/configmap and use it to create deployment/service\nkubectl create secret generic tls-certs --from-file tls/\nkubectl create configmap nginx-frontend-conf --from-file=nginx/frontend.conf\nkubectl create -f deployments/frontend.yaml\nkubectl create -f services/frontend.yaml\n</code></pre> <p>\ud83d\udd18 Step 5: Scale a deployment</p> <p>Use <code>kubectl scale deploy</code> to scale a deployment, the number of replicas can be specified.</p> <pre><code>kubectl scale deployment hello --replicas=5\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#rolling-update","title":"Rolling update","text":"<p>When a deployment is updated with a new version, it creates a new ReplicaSet and slowly increases the number of replicas in the new ReplicaSet as it decreases the replicas in the old ReplicaSet.</p> <p>\ud83d\udd18 Step 1: trigger rolling update.</p> <p>When updating deployment using <code>kubectl edit deployment</code>, the rolling update can be triggered. </p> <pre><code>kubectl edit deployment hello\nkubectl get replicaset ## see the new replicaset\n    #hello-5d9887b889      5         5         4       50s\n    #hello-64fc59fc76      0         0         0       6m6s\n\nkubectl rollout history deployment/hello ## set the rollout history\n\n    # hello\n    # deployment.apps/hello \n    # REVISION  CHANGE-CAUSE\n    # 1         &lt;none&gt;\n    # 2         &lt;none&gt;\n</code></pre> <p>\ud83d\udd18 Step 2: pause a rolling update</p> <p>Rolling update can be paused.</p> <pre><code>kubectl rollout pause deployment/hello\nkubectl rollout status deployment/hello\n</code></pre> <p>\ud83d\udd18 Step 3: resume a rolling update</p> <p>The rollout is paused which means that some pods are at the new version and some pods are at the older version. The rolling process is resumed by: </p> <pre><code>kubectl rollout resume deployment/hello\n</code></pre> <p>\ud83d\udd18 Step 4: rollback an update</p> <p><code>undo</code> command will help to rollback to last stage.</p> <pre><code>kubectl rollout undo deployment/hello\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#canary-deployments","title":"Canary deployments","text":"<p>Canary deployments allow you to release a change to a small subset of your users to mitigate risk associated with new releases.</p> <p>In the <code>hello-canary.yaml</code>, specified the app name as <code>hello</code> but track as <code>canary</code>. This will create another deployment <code>hello-canary</code>. But in the service, it use <code>app:hello</code> selector which will match pods in both the prod deployment and canary deployment.  </p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n        track: canary\n        # Use ver 2.0.0 so it matches version on service selector\n        version: 2.0.0\n</code></pre> <p>If want user to stay with same deployment, specified <code>sessionAffinity</code> in the <code>service</code> file. All clients with the same IP address will have their requests sent to the same version of the hello application.</p> <pre><code>kind: Service\napiVersion: v1\nmetadata:\n  name: \"hello\"\nspec:\n  sessionAffinity: ClientIP\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#blue-green-deployments","title":"Blue-green deployments","text":"<p>Some time we want load balancers to point to that new version only after it has been fully deployed. </p> <p>Kubernetes achieves this by creating two separate deployments. Once the new \"green\" version is up and running, you'll switch over to using that version by updating the Service.</p> <p>Note</p> <p>When using \"blue-green\" deployments, at least 2x the resources in cluster is necessary.</p>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#3-kubernetes-engine-pipeline-using-cloud-build","title":"3. Kubernetes Engine Pipeline using Cloud Build","text":"<p>Objectives</p> <ul> <li>Create Kubernetes Engine clusters</li> <li>Create Cloud Source Repositories</li> <li>Trigger Cloud Build from Cloud Source Repositories</li> <li>Automate tests and publish a deployable container image via Cloud Build</li> <li>Manage resources deployed in a Kubernetes Engine cluster via Cloud Build</li> </ul>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#create-the-git-repositories-in-cloud-source-repositories","title":"Create the Git repositories in Cloud Source Repositories","text":"<p>\ud83d\udd18 Step 1: create two gi repositories in GC.</p> <pre><code>gcloud source repos create hello-cloudbuild-app\ngcloud source repos create hello-cloudbuild-env\n</code></pre> <p>Use this repository as a base, add <code>hello-cloudbuild-app</code> repo as its remote.</p> <pre><code>git clone https://github.com/GoogleCloudPlatform/gke-gitops-tutorial-cloudbuild hello-cloudbuild-app\n\ncd ~/hello-cloudbuild-app\n# add REGION to each yaml file\nsed -i \"s/us-central1/$REGION/g\" cloudbuild.yaml\n\ngit remote add google \"https://source.developers.google.com/p/${PROJECT_ID}/r/hello-cloudbuild-app\"\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#create-a-container-image-with-cloud-build","title":"Create a container image with Cloud Build","text":"<p>Using the following docker file to create a container.</p> <pre><code>FROM python:3.7-slim\nRUN pip install flask\nWORKDIR /app\nCOPY app.py /app/app.py\nENTRYPOINT [\"python\"]\nCMD [\"/app/app.py\"]\n</code></pre> <p>To build the container with Cloud Build and store it in Artifact Registry, using <code>gcloud builds</code>.</p> <pre><code>cd ~/hello-cloudbuild-app\nCOMMIT_ID=\"$(git rev-parse --short=7 HEAD)\"\ngcloud builds submit --tag=\"${REGION}-docker\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#create-the-continuous-integration-ci-pipeline","title":"Create the Continuous Integration (CI) pipeline","text":"<p>\ud83d\udd18 Step1: create trigger for <code>hello-cloudbuild-app</code>, the event is <code>Push to a branch</code> and the build configuration is file <code>cloudbuild.yaml</code>.</p> <p>\ud83d\udd18 Step2: trigger the event by push new content into <code>hello-cloudbuild-app</code>.</p> <pre><code>cd ~/hello-cloudbuild-app\ngit add .\ngit commit -m \"Type Any Commit Message here\"\ngit push google master\n</code></pre>"},{"location":"DevOps/GCPDevOps/lab3.implement_devops_in_google_cloud.html#create-the-test-environment-and-cd-pipeline","title":"Create the Test Environment and CD pipeline","text":"<p>In CD process, each time a commit is pushed to the <code>candidate</code> branch of the hello-cloudbuild-env repository, the pipeline applies the new version of the manifest to the Kubernetes cluster. If successful, itcopies the manifest over to the production branch.</p> <ul> <li>candidate branch -&gt; deployment attempts</li> <li>production branch -&gt; history of the successful deployments</li> <li>rollback to any previous deployment by re-executing the corresponding build in Cloud Build</li> </ul> <p>\ud83d\udd18 Step1 : Grant Cloud Build access to GKE. IAM policy of Kubernetes Engine Developer Identity need to be added to cloudbuild.</p> <pre><code>gcloud projects add-iam-policy-binding ${PROJECT_NUMBER} \\\n--member=serviceAccount:${PROJECT_NUMBER}@cloudbuild.gserviceaccount.com \\\n--role=roles/container.developer\n</code></pre> <p>\ud83d\udd18 Step 2: create new branches production/candidate.</p> <pre><code>cd ~\ngcloud source repos clone hello-cloudbuild-env\ncd ~/hello-cloudbuild-env\ngit checkout -b production\ngit checkout -b candidate\n</code></pre> <p>Copy original cloudbuild file to <code>hello-cloudbuild-env</code> repo and commit.</p> <pre><code>cp ~/hello-cloudbuild-app/cloudbuild-delivery.yaml ~/hello-cloudbuild-env/cloudbuild.yaml\ngit add .\ngit commit -m \"Create cloudbuild.yaml for deployment\"\n</code></pre> <p>Similar to previous step, create trigger for <code>hello-cloudbuild-env</code>, set only <code>^candidate$</code> branch.</p>"},{"location":"DevOps/GCPDevOps/lab4.build_a_website.html","title":"LAB: Build a Website on Google Cloud","text":""},{"location":"DevOps/GCPDevOps/lab4.build_a_website.html#1-deploy-your-website-on-cloud-run","title":"1. Deploy Your Website on Cloud Run","text":"<p>Objectives</p> <ul> <li>How to build a Docker image using Cloud Build and upload it to Artifact Registry</li> <li>How to deploy Docker images to Cloud Run</li> <li>How to manage Cloud Run deployments</li> <li>How to setup an endpoint for an application on Cloud Run</li> </ul> <p>\ud83d\udd18 Step1:  Clone the source repository</p> <p>The website is a existed repository, first clone it and run it locally.</p> <pre><code>git clone https://github.com/googlecodelabs/monolith-to-microservices.git\ncd ~/monolith-to-microservices\n./setup.sh\ncd ~/monolith-to-microservices/monolith\nnpm start\n</code></pre> <p>\ud83d\udd18 Step2: </p>"},{"location":"DevOps/GCPSec/index.html","title":"GCP: Security Engineer Learning Path","text":"<p>Personal record of the course series Security Engineer Learning Path .</p> <p>A Security Engineer develops, implements, and monitors their organization\u2019s security infrastructure to protect sensitive information against cybercrime.</p>"},{"location":"DevOps/GCPSec/1.perpare.html","title":"Preparing for Your Professional Cloud Security Engineer Journey","text":""},{"location":"DevOps/GCPSec/1.perpare.html#1-secure-identity-and-access-foundation","title":"1. Secure identity and access foundation","text":"<p>Note</p> <ul> <li> configure cloud identity</li> <li> manage service account</li> <li> manage authentication</li> <li> manage and implement authorization controls</li> <li> define their resource hierarchy. </li> </ul> Purpose Tools / Means synchronize on-premises Active Directory users and groups to Cloud Identity Google Cloud Directory Sync Minimum Access Rule create separate service accounts"},{"location":"Leetcode/1207.%20Unique%20Number%20of%20Occurrences.html","title":"1207. Unique Number of Occurrences","text":"<p>Takeaways</p> <p>If the given data has a small range, use array instead of set/map. </p>","tags":["Easy","Hash Table","Array"]},{"location":"Leetcode/1207.%20Unique%20Number%20of%20Occurrences.html#solution-1-unordered_map-unordered_set","title":"Solution 1: unordered_map + unordered_set","text":"<p>Iterate through <code>arr</code>, use hashmap <code>m</code> to record the occurrence number of each element. Then use hashset <code>s</code> to check the uniqueness of the number of occurrence.</p> <pre><code>class Solution {\npublic:\n    bool uniqueOccurrences(vector&lt;int&gt;&amp; arr) {\n        unordered_map&lt;int,int&gt; m;\n        unordered_set&lt;int&gt; s;\n        for(auto&amp;i:arr){\n            if(m.find(i) == m.end()) m[i] =0;\n            else ++m[i];\n        }\n        for(auto [i,p]: m){\n            if(s.find(p) != s.end()) return false;\n            s.insert(p);\n        }\n        return true;\n\n    }\n};\n</code></pre>","tags":["Easy","Hash Table","Array"]},{"location":"Leetcode/1207.%20Unique%20Number%20of%20Occurrences.html#solution-2-array","title":"Solution 2: Array","text":"<p>Because of the Constraints:</p> <ul> <li>1 &lt;= arr.length &lt;= 1000</li> <li>-1000 &lt;= arr[i] &lt;= 1000</li> </ul> <p>The hashmap <code>m</code> can be replaced by an array with 2000 length. Every element is mapped to <code>j = i + 1000</code>.</p> <p>The hashset <code>s</code> can also be replaced by an array with 1001 length, because an element can occurrence at most 1001 times.</p> <pre><code>class Solution {\npublic:\n    bool uniqueOccurrences(vector&lt;int&gt;&amp; arr) {\n        int m[2000] = {0};\n        bool s[1001] = {false};\n        for(auto&amp;i:arr){\n            ++m[i+1000];\n        }\n        for(auto&amp; p: m){\n            if(p==0) continue;\n            if(s[p]) return false;\n            s[p] = true;\n        }\n        return true;\n\n    }\n};\n</code></pre>","tags":["Easy","Hash Table","Array"]},{"location":"Leetcode/134.%20Gas%20Station.html","title":"134. Gas Station","text":"","tags":["Medium","Greedy","Array"]},{"location":"Leetcode/134.%20Gas%20Station.html#solution-1-greedy","title":"Solution 1: Greedy","text":"<ul> <li>The cur can travel a circle if the total amount of gas is greater than cost. <code>int total</code> record the total difference of gas and cost.</li> <li><code>int cur</code> record the amount of gas in the car when it starts at <code>pos</code> and reached current index. If <code>cur &lt; 0</code>, the <code>pos</code> cannot be the starting point, so it is adjust to next index.</li> </ul> <pre><code>class Solution {\npublic:\n    int canCompleteCircuit(vector&lt;int&gt;&amp; gas, vector&lt;int&gt;&amp; cost) {\n        int n = gas.size();\n        int cur = 0, pos = 0, total = 0;\n        for(int i =0; i &lt; n; i++){\n            cur = cur + gas[i] - cost[i]; // when arrive (i+1) the amount of gas\n            total = total + gas[i] - cost[i];\n            if(cur &lt; 0) {\n                pos = i+1;\n                cur = 0;\n            }\n            // if(pos == -1 &amp;&amp; gas[i] - cost[i] &gt;= 0) pos = i;//(i+1)%n;\n\n        }\n\n        return total &lt; 0 ? -1 : pos;\n    }\n};\n</code></pre>","tags":["Medium","Greedy","Array"]},{"location":"Leetcode/135.%20Candy.html","title":"135. Candy","text":""},{"location":"Leetcode/230.%20Kth%20Smallest%20Element%20in%20a%20BST.html","title":"230. Kth Smallest Element in a BST","text":"","tags":["Medium","Tree","BST"]},{"location":"Leetcode/230.%20Kth%20Smallest%20Element%20in%20a%20BST.html#solution-1-dfs-in-preorder","title":"Solution 1: DFS in preorder","text":"<p>For BST, doing preorder-traversal can have a ordered list. When travel to the kth node, record it and return.</p> <pre><code>class Solution {\npublic:\n    void dfs(TreeNode* root, int&amp; k, int&amp; tar){\n        if(!root) return;\n        dfs(root -&gt; left, k, tar);\n        --k;\n        if(k == 0) {\n            tar = root -&gt; val;\n            return;\n        }\n        dfs(root -&gt; right, k, tar);\n\n    }\n    int kthSmallest(TreeNode* root, int k) {\n        int ans = 0, a = k;\n        dfs(root, a, ans);\n        return ans;\n    }\n};\n</code></pre>","tags":["Medium","Tree","BST"]},{"location":"Leetcode/380.%20Insert%20Delete%20GetRandom%20O%281%29.html","title":"380. Insert Delete GetRandom O(1)","text":"","tags":["Medium","Hash Table","Array"]},{"location":"Leetcode/380.%20Insert%20Delete%20GetRandom%20O%281%29.html#solution-1-use-unordered_set","title":"Solution 1: Use unordered_set","text":"<p>Use <code>unordered_set</code> as the base structure.</p> <ul> <li><code>insert</code> and <code>remove</code> calls <code>find</code> method to test the existence of <code>val</code>.</li> <li><code>getRandom</code> choose a random index in the range of <code>(0,size()-1)</code>, and use iterator to get the value;</li> </ul> <pre><code>class RandomizedSet {\npublic:\n    unordered_set&lt;int&gt; s;\n\n    RandomizedSet() {\n    }\n\n    bool insert(int val) {\n        if(s.find(val) == s.end()){\n            s.insert(val);\n            return true;\n        }\n        return false;\n    }\n\n    bool remove(int val) {\n        auto it = s.find(val);\n        if(it == s.end()) return false;\n        else{\n            it = s.erase(it);\n            return true;\n        }\n    }\n\n    int getRandom() {\n        int random = rand() % s.size();\n        auto it = s.begin();\n        it =  next(it, random);\n        return *it;\n    }\n};\n</code></pre>","tags":["Medium","Hash Table","Array"]},{"location":"Leetcode/380.%20Insert%20Delete%20GetRandom%20O%281%29.html#solution-2-use-unordered_map-vector","title":"Solution 2: Use unordered_map + vector","text":"<p><code>getRandom</code> function in solution 1 takes \\(o(n)\\) time because the iterator takes at most \\(n\\) times to travel to its destination. An additional <code>vector</code> is added for quicker fetch of value.</p> <p>In the following code:</p> <ul> <li><code>m</code> stores the pairs of <code>(val, i)</code>, while in the vector <code>v[i] = val</code>.</li> <li>when <code>insert</code> a new element, current index <code>i</code> is incremented by 1. New value is inserted in both <code>m</code> and <code>v</code>.</li> <li>when <code>remove</code> a existed element , the current back element of <code>v</code> is swapped with the deleted one, and the pair in <code>m</code> is updated according. </li> <li>Example: When <code>v={a,b,c}, m = {(a,0),(b,1),(c,2)}, i = 2</code>, try to remove <code>val</code> = <code>a</code> get the result of <code>v={c,b}, m = {(b,1), (c,0)}, i = 1</code>.</li> <li><code>getRandom</code> only need to choose a random value from the vector, while is done by indexing \\(\\to o(1)\\).</li> </ul> <pre><code>class RandomizedSet {\npublic:\n    unordered_map&lt;int,int&gt; m;\n    vector&lt;int&gt; v;\n    int i ;\n    RandomizedSet() {\n        i = 0;\n    }\n\n    bool insert(int val) {\n        if(m.find(val) != m.end()) return false;\n        v.push_back(val);\n        m[val] = i++;\n        return true;\n    }\n\n    bool remove(int val) {\n        if(m.find(val) == m.end()) return false;\n        auto x = v.back();\n        v[m[val]] = x;\n        v.pop_back();\n        m[x] = m[val];\n        m.erase(val);\n        i --;\n        return true;\n    }\n\n    int getRandom() {\n        return v[rand() % v.size()];\n    }\n};\n</code></pre>","tags":["Medium","Hash Table","Array"]},{"location":"Leetcode/530.%20Minimum%20Absolute%20Difference%20in%20BST.html","title":"530. Minimum Absolute Difference in BST","text":"","tags":["Easy","Tree","BST"]},{"location":"Leetcode/530.%20Minimum%20Absolute%20Difference%20in%20BST.html#solution-1-dfs-to-vector","title":"Solution 1: DFS to vector","text":"<ul> <li>Use dfs to go through the tree, and collect the result to a vector.</li> <li>Sort the vector and get the minimum difference</li> </ul> <pre><code>class Solution {\npublic:\n    void dfs(TreeNode* root, vector&lt;int&gt;&amp; v){\n        if(!root) return;\n        v.push_back(root -&gt; val);\n        dfs(root-&gt;left,v);\n        dfs(root-&gt;right, v);\n    }\n    int getMinimumDifference(TreeNode* root) {\n        vector&lt;int&gt; v;\n        dfs(root, v);\n        sort(v.begin(), v.end());\n        int minn = INT_MAX;\n        for(int i = 0; i &lt; v.size() - 1; i ++) minn = min(minn, v[i+1] - v[i]);\n        return minn;\n    }\n};\n</code></pre>","tags":["Easy","Tree","BST"]},{"location":"Leetcode/530.%20Minimum%20Absolute%20Difference%20in%20BST.html#solution-2-dfs-with-o1-space","title":"Solution 2: DFS with O(1) space","text":"<p>Solution 1 ignore the fact the the tree is a BST. If doing postorder traversal, we can get the same result of a reverse-sorted vector. Since we only need the difference of neighbor, we use <code>next</code> to record the next bigger neighbor, and <code>minn</code> to record the minimum difference.</p> <pre><code>class Solution {\npublic:\n    void dfs(TreeNode* root, int&amp; minn, int&amp; next){\n        if(!root) return;\n        dfs(root-&gt;right, minn, next);\n        minn = min(minn, next - root -&gt; val);\n        next = root -&gt; val;\n        dfs(root-&gt;left, minn, next);\n\n    }\n    int getMinimumDifference(TreeNode* root) {\n        int next = INT_MAX, minn = INT_MAX;\n        dfs(root, minn, next);\n        return minn;\n    }\n};\n</code></pre> <p>Note</p> <p>If travel in preorder traversal(left-root-right), then we need to add a <code>abs</code> operation. In preorder, if define <code>prev = INT_MAX</code>, then it cannot deal with the first node. <pre><code>void dfs(TreeNode* root, int&amp; minn, int&amp; prev){\n    if(!root) return;\n    dfs(root-&gt;left, minn, prev);\n    minn = min(minn, abs(root -&gt; val - prev));\n    prev = root -&gt; val;\n    dfs(root-&gt;right, minn, prev);\n}\n</code></pre></p>","tags":["Easy","Tree","BST"]},{"location":"Leetcode/98.%20Validate%20Binary%20Search%20Tree.html","title":"98. Validate Binary Search Tree","text":"","tags":["Medium","Tree","BST"]},{"location":"Leetcode/98.%20Validate%20Binary%20Search%20Tree.html#solution-1-dfs-with-long-long","title":"Solution 1: DFS with long long","text":"<p>For BST, doing preorder-traversal can have a ordered list. We can only compare the ordered in one direction rather than both. Every time we record the previous node and compare if it is less than next node. We need to use <code>long long</code> to deal with the edge case [-2147483648].</p> <pre><code>class Solution {\npublic:\n    bool dfs(TreeNode* root, long long &amp; prev){\n        if(!root) return true;\n        bool ans = true;\n        ans &amp;= dfs(root -&gt; left, prev);\n        if(prev &gt;= root -&gt; val) return false;\n        prev = root-&gt; val;\n        ans &amp;= dfs(root -&gt; right, prev);\n        return ans;\n    }\n    bool isValidBST(TreeNode* root) {\n        long long prev = INT_MIN;\n        prev--;\n        return dfs(root, prev);\n    }\n};\n</code></pre>","tags":["Medium","Tree","BST"]},{"location":"Leetcode/tags.html","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"Leetcode/tags.html#array","title":"Array","text":"<ul> <li>1207. Unique Number of Occurrences</li> <li>134. Gas Station</li> <li>380. Insert Delete GetRandom O(1)</li> </ul>"},{"location":"Leetcode/tags.html#bst","title":"BST","text":"<ul> <li>230. Kth Smallest Element in a BST</li> <li>530. Minimum Absolute Difference in BST</li> <li>98. Validate Binary Search Tree</li> </ul>"},{"location":"Leetcode/tags.html#easy","title":"Easy","text":"<ul> <li>1207. Unique Number of Occurrences</li> <li>530. Minimum Absolute Difference in BST</li> </ul>"},{"location":"Leetcode/tags.html#greedy","title":"Greedy","text":"<ul> <li>134. Gas Station</li> </ul>"},{"location":"Leetcode/tags.html#hash-table","title":"Hash Table","text":"<ul> <li>1207. Unique Number of Occurrences</li> <li>380. Insert Delete GetRandom O(1)</li> </ul>"},{"location":"Leetcode/tags.html#medium","title":"Medium","text":"<ul> <li>134. Gas Station</li> <li>230. Kth Smallest Element in a BST</li> <li>380. Insert Delete GetRandom O(1)</li> <li>98. Validate Binary Search Tree</li> </ul>"},{"location":"Leetcode/tags.html#tree","title":"Tree","text":"<ul> <li>230. Kth Smallest Element in a BST</li> <li>530. Minimum Absolute Difference in BST</li> <li>98. Validate Binary Search Tree</li> </ul>"},{"location":"Supercomputing/1.introduction_to_HPC.html","title":"1. Introduction to HPC","text":""},{"location":"Supercomputing/1.introduction_to_HPC.html#11-what-is-high-performance-computing","title":"1.1 What is high-performance computing?","text":"<p>Supercomputer is defined as using computing power that is vastly larger than available in a typical desktop computer.</p>"},{"location":"Supercomputing/1.introduction_to_HPC.html#paradigms-of-science","title":"Paradigms of science","text":"<p>\ud83d\udd18 The three paradigms of science</p> <ul> <li>Experimental science: observations and measurements</li> <li>Theoretical science: develops models which fit or \u201cexplain\u201d measurements</li> <li>Computational science: design, implementation, and use of mathematical models to analyse and solve scientific problems via computer simulations and numerical analysis</li> </ul> <p>\ud83d\udd18 Data science: new paradigm</p> <p>Data science can be defined as a field that uses various mathematical methods and algorithms to extract knowledge and insight from data.</p> <p>Related to machine learning(probabilistic models) and artificial intelligence(as a system, it learns from itself).</p>"},{"location":"Supercomputing/1.introduction_to_HPC.html#hpc","title":"HPC","text":"<p>Problem with traditional desktop computer</p> <p>Solving the problem which contains enormous computing work with standard computers might take years or be impossible as the problem cannot fit into memory.</p> <p>Compared to single-used measure machines(particle accelerator, radiotelescope...) in different field, supercomputers can be used in multitude of application areas.</p> <ul> <li>Fundamental sciences such as particle physics and cosmology</li> <li>Climate, weather and earth sciences</li> <li>Life sciences and medicine</li> <li>Chemistry and material science</li> <li>Energy, e.g oil and gas exploration and fusion research</li> <li>Engineering, e.g. infrastructure and manufacturing</li> <li>Data analysis</li> <li>Artificial intelligence and machine learning</li> </ul>"},{"location":"Supercomputing/1.introduction_to_HPC.html#12-key-terms","title":"1.2  Key terms","text":"<p>\ud83d\udd18 CPU</p> <ul> <li>central processing unit which performs logical, arithmetic, controlling and input/output operations.</li> <li>Household computers usually have 4-8 cores(processing units), while server processors may have 64 cores.</li> </ul> <p>\ud83d\udd18 GPU</p> <ul> <li>graphics processing unit which specialized in very simple but fast arithmetic.</li> <li>called \"accelerators\" because it accelerates the massive amount of calculations done in HPC.</li> </ul> <p>\ud83d\udd18 Disk : long term storage used by a computer. Slow read and write.</p> <p>\ud83d\udd18 Memory : random access memory (RAM), temporary storage from runtime program data. Fast R/W but wiped after shutting off computer. </p> <p>\ud83d\udd18 Cache : special memory, extremely small and fast.</p> <p>\ud83d\udd18 Node :</p> <ul> <li>Basic block of supercomputer, contains CPUs and memory.</li> <li>equivalent to a desktop computer. In reverse, laptops/desktop can be a node in supercomputer.</li> </ul> <p>\ud83d\udd18 Cluster/Supercomputer : formed by connecting a set of nodes together via a fast network.</p> <p>\ud83d\udd18 Interconnect : high-speed network connecting hundreds or thousands of nodes</p> <p>\ud83d\udd18 Virtualization : simulating the necessary hardware for a guest operating system. Virtualization makes \\(x\\) nodes to be treated as a whole vm.</p> <p>\ud83d\udd18 Virtual Machine : independent computer that is simulated using virtualization technology.</p> <p>\ud83d\udd18 MPI</p> <ul> <li>message passing interface, allowing multiple nodes or cores to communicate with each other, and execute computation in parallel.</li> <li>MPI allocates isolated memory for each task.</li> <li>different levels of abstraction: CPU, nodes...</li> </ul> <p>\ud83d\udd18 OpenMP :</p> <ul> <li>a parallel programming approach that is based on a shared memory model</li> <li>Program is split into multiple tasks which can exchange information by directly R/W to the common shared memory.</li> <li>levels of abstraction: only within a single node</li> </ul>"},{"location":"Supercomputing/1.introduction_to_HPC.html#3-examples-of-hpc-applications","title":"3. Examples of HPC applications","text":"<ul> <li>Collisions of nanoparticles</li> <li>Spreading of aerosols in the air</li> <li>Climate change</li> <li>Natural language processing</li> <li>Cancer diagnosis</li> </ul>"},{"location":"Supercomputing/2.what_is_a_supercomputer.html","title":"2. What is a supercomputer?","text":""},{"location":"Supercomputing/2.what_is_a_supercomputer.html#21-history-of-supercomputing","title":"2.1 History of supercomputing","text":"<ul> <li>gears, levers, curved plates computer, not for programming</li> <li>1945 ENIAC, electronic programmable general-purpose computer</li> <li>1964 CDC 6600, term of supercomputer created</li> <li>1990s, supercomputer based more on commodity processors.</li> <li>2005, no longer follows Moore's law. Instead performance has been increased by adding multiple cores to a single CPU.</li> </ul>"},{"location":"Supercomputing/2.what_is_a_supercomputer.html#22-supercomputing-in-finland-and-at-csc","title":"2.2 Supercomputing in Finland and at CSC","text":""},{"location":"Supercomputing/2.what_is_a_supercomputer.html#23-modern-supercomputer","title":"2.3 Modern supercomputer","text":""},{"location":"Supercomputing/2.what_is_a_supercomputer.html#components-of-high-performance-computing","title":"Components of High Performance Computing","text":"<p>Modern supercomputers are built using the same basic elements, such as CPUs(processors), GPU, memory, and disk, found in desktop computers. It also utilizes interconnect.</p> <p>A supercomputer gets its power from all these CPU cores and GPUs working together simultaneously \u2013 working in parallel.</p>"},{"location":"Supercomputing/2.what_is_a_supercomputer.html#24-supercomputer-performance","title":"2.4 Supercomputer performance","text":"<p>\ud83d\udd18 Measurement Standard:  floating-point operations per second (FLOP/s)</p> <p>\ud83d\udd18 Theoretical Computing Power of a CPU core: \\(\\text{clock speed} * \\text{FLOP/s}\\).</p> <p>\ud83d\udd18 Real performance is less than theoretical capability. (bounded by speed of CPU access to the memory; intercommunicate with other nodes)</p> <p>\ud83d\udd18 Mahti has a theoretical peak performance of 7.5 PFLOP/s. LUMI has 428.70 PFLOP/s.</p>"},{"location":"Supercomputing/2.what_is_a_supercomputer.html#25-data-storage","title":"2.5 Data storage","text":"<p>Storage is an important factor of supercomputer performance.</p> <p>\ud83d\udd18 Two common types of disk drives</p> <ul> <li>Hard disk drives (HDD) that use one or more rotating discs and rely on magnetic storage. [Cheap, slow, used for main storage]</li> <li>Solid-state drives (SSD) that have no moving mechanical parts but use flash memory like the one in a USB flash drive. [Expensive, fast, used for high-speed storage]</li> </ul> <p>\ud83d\udd18 Parallel file system: applications R/W multiple disks simultaneously; multiple users to utilize the same storage simultaneously. Lustre, GPFS, BeeGFS, and Ceph... </p> <p>\ud83d\udd18 Mahti has a storage of 8.7 petabytes (PB) or 8,700 terabytes (TB), equivalent to over 170,000 Blu-rays.</p>"},{"location":"Supercomputing/2.what_is_a_supercomputer.html#26-hpc-and-cloud-computing","title":"2.6 HPC and cloud computing","text":"<p>Problem with massively parallel processing</p> <ul> <li>The users are limited to the available software stack and must wait for the specific resources to be free. </li> <li>They are also limited to a fixed available storage capacity.</li> </ul> <p>\ud83d\udd18 Cloud computing: User access to a pool of configurable computing resources available on demand.</p> <p>\ud83d\udd18 Advantage of cloud computing:</p> <ul> <li>on-demand self-service (automation, everything provided by the service provider)</li> <li>resilience and elasticity (no data loss or downtimes in case of hardware failure)</li> <li>flexibility and scalability (for the user, resources are seemingly unlimited)</li> </ul> <p>\ud83d\udd18 Three common kinds of cloud resources: IaaS, PaaS, SaaS</p>"},{"location":"Supercomputing/2.what_is_a_supercomputer.html#27-top-supercomputers","title":"2.7 Top supercomputers","text":"<ul> <li>Fugaku, Japan(2nd)</li> <li>Summit, USA</li> <li>Sunway TaihuLight, China\ud83d\ude00</li> <li>LUMI, Finland(3rd)</li> <li>Frontier, USA(1st), also most energy-efficient</li> </ul>"},{"location":"Supercomputing/2.what_is_a_supercomputer.html#29-introduction-to-lumi","title":"2.9 Introduction to LUMI","text":"<ul> <li>429 petaflop/s</li> <li>1.5 million ordinary laptops</li> <li>one of the world's most advanced platforms for artificial intelligence</li> <li>MI250X GPUs</li> <li>a storage of 117 petabytes and an aggregated I/O bandwidth of two terabytes per second</li> <li>very fast Cray Slingshot interconnect with 200 Gbit/s performance, the global bandwidth of the LUMI-GPU partition is 160 TB/s</li> <li>150 m2 of floor space; 150 000 kilograms (150 metric tons).</li> </ul>"},{"location":"Supercomputing/3.running_and_using_a_supercomputer.html","title":"3. Running and using a supercomputer","text":""},{"location":"Supercomputing/3.running_and_using_a_supercomputer.html#31-hpc-centers","title":"3.1 HPC centers","text":""},{"location":"Supercomputing/3.running_and_using_a_supercomputer.html#32-energy-and-cooling","title":"3.2 Energy and cooling","text":"<p>\ud83d\udd18 Energy consumption is one of the biggest challenges when designing and building more powerful supercomputers</p> <p>\ud83d\udd18Two most common approaches of cooling</p> <ul> <li>air cooling, where a steady airflow is run through the supercomputer</li> <li>liquid cooling, where water or other suitable liquid is circulated through the system carrying the heat away.</li> </ul>"},{"location":"Supercomputing/3.running_and_using_a_supercomputer.html#33-how-to-buy-a-supercomputer","title":"3.3 How to buy a supercomputer?","text":"<ul> <li>each one customized and unique.</li> <li>computing centers typically buy technology that will only be available later, sometimes even after a few years.(predict the performance of a system that does not yet exist)</li> <li>request for proposals (RFP) is prepared with all relevant details and requirements.</li> </ul>"},{"location":"Supercomputing/3.running_and_using_a_supercomputer.html#34-how-to-access-a-supercomputer","title":"3.4 How to access a supercomputer?","text":"<p>\ud83d\udd18 Access ways:</p> <ul> <li>via command line.</li> <li>web browser-based access(for example puhti.css.fi)</li> </ul> <p>\ud83d\udd18 batch system then takes care of scheduling the distribution of the resources fairly; not monopolize the system</p> <p>\ud83d\udd18 CSC provides free access to researchers and sometimes also for students with an affiliation to Finnish higher education institutions.</p>"},{"location":"Supercomputing/3.running_and_using_a_supercomputer.html#35-operating-systems-in-supercomputers","title":"3.5 Operating systems in supercomputers","text":"<ul> <li>supercomputers -&gt; Linux</li> </ul>"},{"location":"Supercomputing/4.parallel_computing_concepts.html","title":"4. Parallel computing concepts","text":""},{"location":"Supercomputing/4.parallel_computing_concepts.html#41-computing-in-parallel","title":"4.1 Computing in parallel","text":"<ul> <li>split a computational problem into smaller subproblems.</li> </ul>"},{"location":"Supercomputing/mpi.html","title":"Message Passing Interface(MPI)","text":""},{"location":"Supercomputing/mpi.html#1-introduction","title":"1. Introduction","text":"<p>\ud83d\udd18 What is MPI?</p> <p>Programs pass messages through some processes in order to finish parallel tasks. In practice, it's easy to implement parallel tasks by MPI. For example manager process can send a message to describe a new job, and assign the job to the latter.</p> <p>\ud83d\udd18 Concepts in MPI</p> <ul> <li>communicator: define a set of processes which can send message to each other</li> <li>rank: inside a communicator, each of the process has a number(called rank). Processes communicate explicitly by specifying ranks.</li> <li>tag: each message has its unique tag. </li> </ul>"},{"location":"Supercomputing/mpi.html#2-hello-world","title":"2. Hello-world","text":"<p>Take Away</p> <ul> <li><code>MPI_Init(&amp;argc, &amp;argv)</code>: init the MPI environment. All the global variable and local variable will be created(communicator, ranks...).</li> <li><code>MPI_Comm_size(MPI_Comm communicator, int* size)</code>: get the size of communicator.</li> <li><code>MPI_Comm_rank()</code>: get the rank of current process.</li> <li><code>MPI_Get_processor_name()</code>: get name of processor.</li> <li><code>MPI_Finalize()</code>: clean MPI environment. After this no other MPI command can be used.</li> <li><code>MPI_COMM_WORLD</code>: MPI automatically generate communicator.</li> <li><code>mpicc -o my_mpi_exe test.c</code>: compile.</li> <li><code>mpiexec -n 4 ./my_mpi_exe</code>: run program with 4 processes.</li> <li></li> </ul>"},{"location":"Supercomputing/mpi.html#21-program","title":"2.1 Program","text":"Code Local Result Puhti Result <pre><code>#include &lt;stdio.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;mpi.h&gt;\n\nint main(int argc, char *argv[])\n{\n    // TODO: say hello! in parallel\n    MPI_Init(&amp;argc,&amp;argv);\n\n    // Get the number of processes\n    int n;\n    MPI_Comm_size( MPI_COMM_WORLD , &amp; n);\n\n    // Get the rank of processes\n    int rank;\n    MPI_Comm_rank( MPI_COMM_WORLD ,&amp; rank);\n\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\n    int name_len;\n    MPI_Get_processor_name( processor_name , &amp;name_len);\n    printf(\"Hello world from processor %s, rand %d out of %d processor\\n\", processor_name, rank, n);\n\n    MPI_Finalize();\n    return 0;\n}\n</code></pre> <pre><code>[xiezhong@puhti-login11 mpi]$ mpiexec -n 4 ./hello-world/hello_world.exe \nMPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found\nMPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found\nMPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found\nMPI startup(): Warning: I_MPI_PMI_LIBRARY will be ignored since the hydra process manager was found\nHello world from processor puhti-login11.bullx, rand 0 out of 4 processor\nHello world from processor puhti-login11.bullx, rand 1 out of 4 processor\nHello world from processor puhti-login11.bullx, rand 2 out of 4 processor\nHello world from processor puhti-login11.bullx, rand 3 out of 4 processor\n</code></pre> <p><pre><code>[xiezhong@puhti-login11 mpi]$ squeue -u $USER\n         JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n      20170858     small hello-wo xiezhong  R       0:11      1 r02c32\n</code></pre> Here is the result in file <code>slurm-20170858.out</code> <pre><code>Hello world from processor r02c32.bullx, rand 1 out of 4 processor\nHello world from processor r02c32.bullx, rand 2 out of 4 processor\nHello world from processor r02c32.bullx, rand 3 out of 4 processor\nHello world from processor r02c32.bullx, rand 0 out of 4 processor\n</code></pre></p>"},{"location":"Supercomputing/mpi.html#3-message-exchange","title":"3. Message exchange","text":"<p>Take Away</p> <ul> <li><code>MPI_Barrier</code>: Synchronization between processes wait until everybody within the communicator reaches the call.</li> <li><code>MPI_Send(void* data, int count, MPI_Datatype datatype, int destination, int tag, MPI_Comm communicator)</code>: send <code>count</code> data of type <code>datatype</code>. The <code>destination</code> is the rank of receiver, and <code>tag</code> belongs to the message.</li> <li><code>MPI_Recv(void* data, int count, MPI_Datatype datatype, int source, int tag, MPI_Comm communicator, MPI_Status* status)</code>: arguments are similar to <code>MPI_Send</code>, with an additional <code>status</code> specifying the status of whether the message is received successfully.</li> <li><code>MPI_Get_count( const MPI_Status *status, MPI_Datatype datatype, int *count )</code>: number of received elements.</li> </ul>"},{"location":"Supercomputing/mpi.html#31-program","title":"3.1 Program","text":"Code Puhti Result <pre><code>if (myid == 0) {\n    MPI_Send(message, size, MPI_INT, 1,1,MPI_COMM_WORLD);\n    MPI_Recv(receiveBuffer, size, MPI_INT, 1,2,MPI_COMM_WORLD, &amp;status);\n    int nrecv;\n    MPI_Get_count(&amp;status, MPI_INT, &amp;nrecv);\n    printf(\"Rank %i received %i elements, first %i\\n\", myid, nrecv, receiveBuffer[0]);\n} else if (myid == 1) {\n    MPI_Send(message, size, MPI_INT, 0,2,MPI_COMM_WORLD);\n    MPI_Recv(receiveBuffer, size, MPI_INT, 0,1,MPI_COMM_WORLD, &amp;status);\n    int nrecv;\n    MPI_Get_count(&amp;status, MPI_INT, &amp;nrecv);\n    printf(\"Rank %i received %i elements, first %i\\n\", myid, nrecv, receiveBuffer[0]);\n}\n</code></pre> <pre><code>Rank 0 received 100 elements, first 1\nRank 1 received 100 elements, first 0\n</code></pre>"},{"location":"Supercomputing/mpi.html#32-question","title":"3.2 Question","text":"<p>Try increasing the message size (e.g. to 100000), recompile and run. What happens?</p> <p>Get the result of time limit <pre><code>slurmstepd: error: *** STEP 20173362.0 ON r07c20 CANCELLED AT 2024-01-22T12:46:27 DUE TO TIME LIMIT ***\n</code></pre></p> <p>What if you reorder the send and receive calls in one of the processes?</p> <p>The program still run and get the correct answer. <pre><code>Rank 0 received 100 elements, first 1\nRank 1 received 100 elements, first 0\n</code></pre></p>"},{"location":"Supercomputing/mpi.html#4-message-chain","title":"4. Message Chain","text":"<p>Take Away</p> <ul> <li><code>MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag, void *recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag, MPI_Comm comm, MPI_Status * status)</code> This function wrap <code>send/recv</code>, but the communicator must be the same.</li> </ul>"},{"location":"Supercomputing/mpi.html#41-program","title":"4.1 Program","text":"Send &amp; Recv Sendrecv Puhti Result <ul> <li>Every sender sends message with tag <code>tag+1</code> to <code>rank+1</code> receiver</li> <li>Every receiver gets message with tag <code>tag</code> from <code>rank-1</code> sender <pre><code>destination = myid &lt; ntasks - 1 ? myid + 1 : MPI_PROC_NULL;\nsource = myid &gt; 0 ? myid - 1 : MPI_PROC_NULL;\nMPI_Send(message.data(), size, MPI_INT, destination, myid+1, MPI_COMM_WORLD);\nprintf(\"Sender: %d. Sent elements: %d. Tag: %d. Receiver: %d\\n\",\n       myid, size, myid + 1, destination);\nMPI_Recv(receiveBuffer.data(), size, MPI_INT, source, myid, MPI_COMM_WORLD,&amp;status);\nprintf(\"Receiver: %d. first element %d.\\n\",\n       myid, receiveBuffer[0]);\n</code></pre></li> </ul> <ul> <li>Use <code>Sendrecv</code> function can wrap two function together. <pre><code>destination = myid + 1; // ignore edge cases \nsource = myid - 1;\nMPI_Sendrecv(message, size, MPI_INT, destination, myid + 1,\n            receiveBuffer, size, MPI_INT, source, MPI_ANY_TAG,\n            MPI_COMM_WORLD, &amp;status);\n</code></pre></li> </ul> <pre><code>Sender: 0. Sent elements: 10000000. Tag: 1. Receiver: 1\nReceiver: 0. first element -1.\nSender: 1. Sent elements: 10000000. Tag: 2. Receiver: 2\nReceiver: 1. first element 0.\nSender: 2. Sent elements: 10000000. Tag: 3. Receiver: 3\nReceiver: 2. first element 1.\nSender: 3. Sent elements: 10000000. Tag: 4. Receiver: -2\nReceiver: 3. first element 2.\nTime elapsed in rank  0:  0.023\nTime elapsed in rank  1:  0.023\nTime elapsed in rank  2:  0.015\nTime elapsed in rank  3:  0.008\n</code></pre>"},{"location":"Supercomputing/mpi.html#42-question","title":"4.2 question","text":"<p>Investigate the timings with different numbers of MPI tasks (e.g. 2, 4, 8, 16, ...)</p> <p>With <code>recv</code> and <code>send</code>:   - 2 nodes -&gt; Time elapsed in rank  0:  0.007   - 4 nodes -&gt; Time elapsed in rank  0:  0.023   - 8 nodes -&gt; Time elapsed in rank  0:  0.119   - 16 nodes -&gt; Time elapsed in rank  0:  0.177   - Some of nodes always spend more time than others, time differs a lot. With <code>Sendrecv</code>:   - 2 nodes -&gt; TIME LIMIT?   - 4 nodes -&gt; Time elapsed in rank  0:  0.042   - 8 nodes -&gt; Time elapsed in rank  0:  0.086   - 16 nodes -&gt; Time elapsed in rank  0:  0.014   - All the node spend almost same time.</p>"},{"location":"Supercomputing/openMP.html","title":"OpenMP","text":""},{"location":"Tools/linux.html","title":"Linux Command Line Tools","text":""},{"location":"Tools/linux.html#0-linux-basic","title":"0. Linux Basic","text":""},{"location":"Tools/linux.html#linux-dictionaries","title":"Linux Dictionaries","text":"Name Usage <code>/proc</code> Not a real file system, but a virtual one. It is maintain by linux kernel and can communicate with kernel. <code>/media</code> contains the mount point of removable devices. They are mounted automatically at insertion."},{"location":"Tools/linux.html#1-file-navigation","title":"1. File &amp; Navigation","text":""},{"location":"Tools/linux.html#view-dictionary-file","title":"View dictionary &amp; File","text":"<p>TakeAway</p> <pre><code>pwd   #Print name of current working directory\ncd    #Change directory\nls    #List directory contents\nhead  #Show first 10 line of a file\ntail  #Show last 10 line of a file\n</code></pre> <p>\ud83d\udd18 Use <code>pwd</code> to get current dictionary</p> <p><code>pwd</code> stands for print working dictionary.</p> <pre><code>$pwd                                 \n/home/oysterpus\n</code></pre> <p>\ud83d\udd18 Use <code>head</code>, <code>tail</code> to view part of the file</p> <p>By default, head/tail output 10 lines. Line</p> <p>\ud83d\udd18 Use <code>more</code>, <code>less</code></p> <p>Once the <code>less</code> program starts, we can view the contents of the file. If the file is longer than one page, we can scroll up and down. To exit less, press <code>q</code>.</p>"},{"location":"Tools/linux.html#create-modify-filedictionary","title":"Create &amp; Modify File/Dictionary","text":"<p>TakeAway</p> <pre><code>touch #Create new file\nshred #Hide file\nnano  #Edit file\n</code></pre> <p>\ud83d\udd18 Use <code>touch</code> to create a new file</p> <pre><code>touch filename\ntouch name1 name2 name3 name4 # every name create a single file\ntouch name{1..10}             # create file with continuous number\ntouch -d tomorrow filename    # -d to specify file create date\n</code></pre> <p>\ud83d\udd18 Use <code>shred</code> to overwrite and hide a file</p> <pre><code>shred filename\n</code></pre> <p>\ud83d\udd18 Use <code>nano</code> and <code>vim</code> to edit a file</p> <p>\ud83d\udd18 Use <code>mkdir</code> to create a new dictionary</p>"},{"location":"Tools/linux.html#compress-file","title":"Compress File","text":"<p>\ud83d\udd18 Use <code>zip</code> to compress file \ud83d\udd18 Use <code>unzip</code> to decompress file</p> <pre><code>zip zip_name filename\nuzip zip_name \n</code></pre>"},{"location":"Tools/linux.html#2-what-is-running-and-how-to-monitorkill","title":"2. What Is Running And How To Monitor/Kill","text":"<p>TakeAway</p> <pre><code>ps       #Report a snapshot of current processes\ntop      #Display tasks\njobs     #List active jobs\nbg       #Place a job in the background\nfg       #Place a job in the foreground\nkill     #Send a signal to a process\nkillall  #Kill processes by name\nshutdown #Shut down or reboot the system\n</code></pre> <p>When a system starts up, the kernel initiates a few of its own activities as  processes and launches a program called <code>init</code>.</p> <p><code>init</code> returns a series of shell scripts called init scripts.</p> <p>A program can launch other programs is expressed in the  process scheme as a parent process producing a child process.</p> <p>Each process is assigned a number called a process ID (PID).</p>"},{"location":"Tools/linux.html#viewing-processes","title":"Viewing Processes","text":"<p>\ud83d\udd18 Use <code>ps</code></p> <pre><code># ps\n  PID TTY          TIME CMD\n 5694 pts/6    00:00:00 sudo\n 5695 pts/6    00:00:00 go\n 5814 pts/6    00:00:00 UTS\n 5818 pts/6    00:00:00 sh\n10120 pts/6    00:00:00 ps\n</code></pre> <ul> <li><code>TTY</code> is short for \u201cteletype\u201d  and refers to the controlling terminal for the process.</li> <li>The <code>TIME</code> field is the amount of CPU time consumed by the process.</li> </ul> <p>\ud83d\udd18 Use <code>pstree</code> Outputs a process list arranged in a tree-like pattern showing the parent-child relationships between processes.</p> <pre><code>init(Ubuntu-22.(1)\u2500\u252c\u2500SessionLeader(333)\u2500\u252c\u2500Relay(335)(334)\u2500\u2500\u2500docker-desktop-(335)\u2500\u252c\u2500{docker-desktop-}(336)\n                   \u2502                    \u2502                                        \u251c\u2500{docker-desktop-}(337)\n                   \u2502                    \u2502                                        \u251c\u2500{docker-desktop-}(338)\n                   \u2502                    \u2502                                        \u251c\u2500{docker-desktop-}(339)\n                   \u2502                    \u2502                                        \u251c\u2500{docker-desktop-}(340)\n(367)\n                   \u2502                    \u2514\u2500Relay(352)(351)\u2500\u2500\u2500docker(352)\u2500\u252c\u2500{docker}(353)\n                   \u2502                                                    \u251c\u2500{docker}(354)\n                   \u2502                                                    \u251c\u2500{docker}(355)\n                   \u2502                                                    \u251c\u2500{docker}(356)\n                   \u2502                    \n                   \u251c\u2500SessionLeader(390)\u2500\u2500\u2500Relay(392)(391)\u2500\u252c\u2500sh(392)\u2500\u2500\u2500sh(393)\u2500\u2500\u2500sh(418)\u2500\u2500\u2500node(422)\u2500\u252c\u2500node(433)\u2500\u252c\u2500zsh(3366)\u2500\u2500\u2500sudo(5693)\u2500\u2500\u2500sudo(5694)\u2500\u2500\u2500go(5695)\u2500\u252c\u2500UTS(5814)\u2500\u252c\u2500sh(5818)\u2500\u2500\u2500pstree(5969)\n                   \u2502                            \n</code></pre> <p>\ud83d\udd18 Use <code>top</code></p> <p>The top program displays a continuously updating (by default, every 3 seconds) display of the system processes listed in order of process activity.</p>"},{"location":"Tools/linux.html#multiple-processes","title":"Multiple processes","text":"<p>\u200b\u591a\u200b\u8fdb\u7a0b\u200b\u95f4\u901a\u4fe1\u200b\u5e38\u7528\u200b\u7684\u200b\u6280\u672f\u624b\u6bb5\u200b\u5305\u62ec\u200b\u5171\u4eab\u5185\u5b58\u200b\u3001\u200b\u6d88\u606f\u200b\u961f\u5217\u200b\u3001\u200b\u4fe1\u53f7\u91cf\u200b.</p> <p>\ud83d\udd18 Use <code>ipcs</code> \u3000\u3000 - <code>ipcs -m</code>\u3000\u3000\u200b\u67e5\u770b\u200b\u7cfb\u7edf\u200b\u5171\u4eab\u5185\u5b58\u200b\u4fe1\u606f\u200b - <code>ipcs -q</code>\u3000\u3000\u200b\u67e5\u770b\u200b\u7cfb\u7edf\u200b\u6d88\u606f\u200b\u961f\u5217\u200b\u4fe1\u606f\u200b - <code>ipcs -s</code>\u3000\u3000\u200b\u67e5\u770b\u200b\u7cfb\u7edf\u200b\u4fe1\u53f7\u91cf\u200b\u4fe1\u606f\u200b, - <code>ipcs [-a]</code>\u3000\u200b\u7cfb\u7edf\u200b\u9ed8\u8ba4\u200b\u8f93\u51fa\u200b\u4fe1\u606f\u200b\uff0c\u200b\u663e\u793a\u200b\u7cfb\u7edf\u200b\u5185\u200b\u6240\u6709\u200b\u7684\u200bIPC\u200b\u4fe1\u606f\u200b</p> <pre><code># ipcs\n------ Message Queues --------\nkey        msqid      owner      perms      used-bytes   messages    \n\n------ Shared Memory Segments --------\nkey        shmid      owner      perms      bytes      nattch     status      \n\n------ Semaphore Arrays --------\nkey        semid      owner      perms      nsems\n</code></pre> <p>\ud83d\udd18 Use <code>ipcrm</code> \u200b\u901a\u8fc7\u200b\u6307\u5b9a\u200bID(<code>[msq/shm/sem][id/key]</code>)\u200b\u5220\u9664\u200bIPC\u200b\u8d44\u6e90\u200b\uff0c\u200b\u540c\u65f6\u200b\u5c06\u200b\u4e0e\u200bIPC\u200b\u5bf9\u8c61\u200b\u5173\u8054\u200b\u7684\u200b\u6570\u636e\u200b\u4e00\u5e76\u200b\u5220\u9664\u200b\uff0c\u200b\u53ea\u6709\u200b\u8d85\u7ea7\u200b\u7528\u6237\u200b\u6216\u200bIPC\u200b\u8d44\u6e90\u200b\u521b\u5efa\u8005\u200b\u80fd\u591f\u200b\u5220\u9664\u200b\u3002</p>"},{"location":"Tools/linux.html#3-know-my-system-and-get-help","title":"3. Know My System and Get Help","text":""},{"location":"Tools/linux.html#what-and-where","title":"What and Where","text":"<p>\ud83d\udd18 Use <code>man</code> to get help message from tools</p> <p>\ud83d\udd18 Use <code>whatis</code> to get a shorter version of <code>man</code></p> <p>\ud83d\udd18 Use <code>apropos</code> to find what tools match your need.</p> <pre><code>apropos -a change user                                    \nchage (1)            - change user password expiry information\nchfn (1)             - change real user name and information\npasswd (1)           - change user password\nucf (1)              - Update Configuration File: preserve user changes in configuration files\n...\n</code></pre>"},{"location":"Tools/linux.html#4-inputoutput-then-redirect","title":"4. Input/Output, then Redirect","text":"<p>TakeAway</p> <pre><code>cat  #Concatenate files\nsort #Sort lines of text\nuniq #Report or omit repeated lines\ngrep #Print lines matching a pattern\nwc   #Print newline, word, and byte counts for each file\nhead #Output the first part of a file\ntail #Output the last part of a file\ntee  #Read from standard input and write to standard output and files\n</code></pre> <p>Standard Output/Input</p> <ul> <li>Programs such as <code>ls</code> actually send their results to a special file called standard output (often expressed as <code>stdout</code>) and their status messages to another file called standard error (<code>stderr</code>).</li> <li><code>stdin</code> default attaches to keyboard</li> </ul>"},{"location":"Tools/linux.html#reading-files","title":"Reading Files","text":"<p>\ud83d\udd18 Use <code>cat</code> to concatenate file</p> <p>The cat command reads one or more files and copies them to standard output.</p> <pre><code>cat filename\ncat movie.mpeg.0* &gt; movie.mpeg # join all split media file together\n</code></pre>"},{"location":"Tools/linux.html#redirection","title":"Redirection","text":"<p>\ud83d\udd18 Use <code>&gt;</code> and <code>&gt;&gt;</code> to redirect output </p> <p><code>&gt;</code> rewrite a file, while <code>&gt;&gt;</code> append the content to the end.</p> <pre><code>ls -l /usr/bin &gt; ls-output.txt\n</code></pre> <p>\ud83d\udd18 Use <code>2&gt;</code> to redirect error</p> <pre><code>ls -l /bin/usr 2&gt; ls-error.txt\n</code></pre> <p>\ud83d\udd18 Use <code>|</code> to pipeline 2 commands</p> <p>Read data from standard input and send to standard output.</p> <pre><code>command1 | command2\n</code></pre>"},{"location":"Tools/linux.html#5-how-disk-store-media","title":"5. How Disk Store Media","text":"<p>Takeaway</p> <pre><code>mount                #Mount a file system\numount               #Unmount a file system\nfsck                 #Check and repair a file system\nfdisk                #Manipulate disk partition table\nmkfs                 #Create a file system\ndd                   #Convert and copy a file\ngenisoimage(mkisofs) #Create an ISO 9660 image file\nwodim(cdrecord)      #Write data to optical storage media\nmd5sum               #Calculate an MD5 checksum\n</code></pre>"},{"location":"Tools/linux.html#mounting-and-unmounting-storage-devices","title":"Mounting and Unmounting Storage Devices","text":"<ul> <li>Mounting: managing a storage device is attaching the device to the file system tree.</li> </ul> <p>A file named <code>/etc/fstab</code> (short for \u201cfile system table\u201d) lists the devices(typically hard disk partitions) that are to be mounted at boot time.</p> <p><code>/etc/fstab</code> contains 6 field:</p> <ul> <li><code>Device</code>: Actual name of a device</li> <li><code>Mount point</code>: The directory where the device is attached to</li> <li><code>File system type</code>: Fourth Extended File System(ext4), FAT16(msdos), FAT32 (vfat), NTFS (ntfs), CD-ROM (iso9660)</li> <li><code>Options</code>: read-only, gid ...</li> <li><code>Frequency</code>: backed up action when <code>dump</code> command</li> <li><code>Order</code>: checked with the <code>fsck</code> command</li> </ul> <pre><code>LABEL=/12       /        ext4   defaults       1 1\nLABEL=/home     /home    ext4   defaults       1 2\nLABEL=/boot     /boot    ext4   defaults       1 2\ntmpfs           /dev/shm tmpfs  defaults       0 0\ndevpts          /dev/pts devpts gid=5,mode=620 0 0\nsysfs           /sys     sysfs  defaults       0 0\nproc            /proc    proc   defaults       0 0\nLABEL=SWAP-sda3 swap     swap   defaults       0 0\n</code></pre> <p>\ud83d\udd18 Use <code>mount</code></p> <p><code>mount</code> command is used to view the file system that currently mounted.</p> <pre><code>&gt; mount                         \nnone on /mnt/wsl type tmpfs (rw,relatime)\nnone on /usr/lib/wsl/drivers type 9p (ro,nosuid,nodev,noatime,dirsync,aname=drivers;fmask=222;dmask=222,mmap,access=client,msize=65536,trans=fd,rfd=7,wfd=7)\nnone on /usr/lib/wsl/lib type overlay (rw,relatime,lowerdir=/gpu_lib_packaged:/gpu_lib_inbox,upperdir=/gpu_lib/rw/upper,workdir=/gpu_lib/rw/work)\n/dev/sdc on / type ext4 (rw,relatime,discard,errors=remount-ro,data=ordered)\n</code></pre> <p>The format of the listing is as follows:</p> <ul> <li><code>device</code> on <code>mount_point</code> type <code>filesystem_type</code> (options).</li> </ul> <p>\ud83d\udd18 Use <code>umount</code> &amp; <code>mount</code> to change mount point</p> <pre><code>umount /dev/sdc\nmkdir /mnt/cdrom\nmount -t iso9660 /dev/sdc /mnt/cdrom\n</code></pre> <p>Use <code>-t</code> to specify the file system type, followed by <code>[src] [dst]</code>.</p>"},{"location":"Tools/linux.html#creating-new-file-systems","title":"Creating New File Systems","text":""},{"location":"Tools/linux.html#testing-and-repairing-file-systems","title":"Testing and Repairing File Systems","text":""},{"location":"Tools/linux.html#moving-data-directly-to-and-from-devices","title":"Moving Data Directly to and from Devices","text":""},{"location":"Tools/linux.html#7-network","title":"7. Network","text":"<p>Takeaway</p> <pre><code>ping        #Send an ICMP ECHO_REQUEST to network hosts\ntraceroute  #Print the route packets trace to a network host\nip          #Show/manipulate routing, devices, policy routing, and tunnels\nnetstat     #Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships\nftp         #Internet file transfer program\nwget        #Non-interactive network downloader\nssh         #OpenSSH SSH client (remote login program)\n</code></pre>"},{"location":"Tools/linux.html#examining-and-monitoring-a-network","title":"Examining and Monitoring a Network","text":"<p>\ud83d\udd18 Use <code>ping</code></p> <p>\ud83d\udd18 Use <code>traceroute</code></p> <p>\ud83d\udd18 Use <code>ip</code>(<code>ifconfig</code>)</p> <p><code>ip</code> is the newer version of <code>ifconfig</code>.</p> <p>With ip, we can examine a system\u2019s network interfaces and routing table.</p> <pre><code>#ip a             \n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1472 qdisc mq state UP group default qlen 1000\n    link/ether 00:15:5d:e9:80:d4 brd ff:ff:ff:ff:ff:ff\n    inet 172.20.209.32/20 brd 172.20.223.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::215:5dff:fee9:80d4/64 scope link \n       valid_lft forever preferred_lft forever\n</code></pre> <ul> <li>2 interfaces: <code>lo</code> is loopback interface,<code>eth0</code> is the Ethernet interface.</li> <li>In the first line of each interface, the <code>UP</code> words indicates the network interface is enabled.</li> <li>A valid IP address verifies that the DHCP is working.</li> </ul>"},{"location":"Tools/linux.html#transporting-files-over-a-networkftp-wget-curl","title":"Transporting Files over a Network(<code>ftp</code>, <code>wget</code>, <code>curl</code>)","text":"<p>\ud83d\udd18 Use <code>ftp</code> to transfer file via File Transfer Protocol</p> <p>URIs starting with the protocol <code>ftp://.</code></p> <p>\ud83d\udd18 Use <code>curl</code> to download a file from an url and redirect to another filename</p>"},{"location":"Tools/linux.html#secure-communication-with-remote-hostsssh","title":"Secure Communication with Remote Hosts(<code>ssh</code>)","text":"<p>\ud83d\udd18 Use <code>ssh</code></p> <p>SSH consists of two parts. An SSH server runs on the remote host, listening for incoming connections on port 22, while an SSH client is used on the local system to communicate with the remote server.</p> <ul> <li>It authenticates that the remote host is who it says it is (thus preventing so-called MITM attacks).</li> <li>It encrypts all of the communications between the local and remote hosts.</li> </ul> <pre><code>$ ssh username@1.1.1.1\n</code></pre>"},{"location":"Tools/linux.html#8-how-am-i-what-can-i-do","title":"8. How AM I? What Can I DO?","text":"<p>Takeaway</p> <pre><code>id        #Display user identity\nchmod     #Change a file\u2019s mode\numask     #Set the default file permissions\nsu        #Run a shell as another user\nsudo      #Execute a command as another user\nchown     #Change a file\u2019s owner\nchgrp     #Change a file\u2019s group ownership\npasswd    #Change a user\u2019s password\n</code></pre>"},{"location":"Tools/linux.html#owners-group-members-and-everybody-else","title":"Owners, Group Members, and Everybody Else","text":"<p>\ud83d\udd18 Use <code>id</code> Find out identity. Include 3 fields: <code>uid</code>, <code>gid</code>, <code>groups</code>.</p> <pre><code>uid=1000(oysterpus) gid=1000(oysterpus) groups=1000(oysterpus),4(adm),20(dialout),24(cdrom),25(floppy),27(sudo),29(audio),30(dip),44(video),46(plugdev),116(netdev),1001(docker)\n</code></pre> <p>User accounts are defined in the <code>/etc/passwd</code> file, and groups are defined in the <code>/etc/group</code> file.</p>"},{"location":"Tools/linux.html#9-miscellaneous","title":"9. Miscellaneous","text":""},{"location":"Tools/markdown.html","title":"Markdown / Mkdocs","text":""},{"location":"Tools/markdown.html#markdown-code-block","title":"Markdown Code block","text":"bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"Tools/markdown.html#markdown-link-and-url","title":"Markdown Link and URL","text":"<p>Note</p> <ol> <li>\u200b\u94fe\u63a5\u200b\u6587\u672c\u200b\u653e\u5728\u200b\u4e2d\u62ec\u53f7\u200b\u5185\u200b</li> <li>\u200b\u94fe\u63a5\u200b\u5730\u5740\u200b\u653e\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u62ec\u53f7\u200b\u4e2d\u200b</li> <li>\u200b\u94fe\u63a5\u200btitle\u200b\u53ef\u200b\u9009\u200b\uff08\u200b\u5f53\u200b\u9f20\u6807\u60ac\u505c\u200b\u5728\u200b\u94fe\u63a5\u200b\u4e0a\u200b\u65f6\u4f1a\u200b\u51fa\u73b0\u200b\u7684\u200b\u6587\u5b57\u200b\uff09</li> <li>\u200b\u5c16\u62ec\u53f7\u200b\u7b80\u5355\u200b\u5904\u7406\u200bURL/email</li> <li>\u200b\u5f15\u7528\u200b\u7c7b\u578b\u200b\u94fe\u63a5\u200b</li> </ol> <p>\u200b\u8d85\u94fe\u63a5\u200bMarkdown\u200b\u8bed\u6cd5\u200b\u4ee3\u7801\u200b\uff1a \u200b\u8d85\u94fe\u63a5\u200b\u663e\u793a\u200b\u540d\u200b <code>[\u200b\u8d85\u94fe\u63a5\u200b\u663e\u793a\u200b\u540d\u200b](\u200b\u8d85\u94fe\u63a5\u200b\u5730\u5740\u200b \"\u200b\u8d85\u94fe\u63a5\u200btitle\")</code></p> <p>\u200b\u5bf9\u5e94\u200b\u7684\u200bHTML\u200b\u4ee3\u7801\u200b\uff1a\u200b\u8d85\u94fe\u63a5\u200b\u663e\u793a\u200b\u540d\u200b <code>&lt;a href=\"\u200b\u8d85\u94fe\u63a5\u200b\u5730\u5740\u200b\" title=\"\u200b\u8d85\u94fe\u63a5\u200btitle\"&gt;\u200b\u8d85\u94fe\u63a5\u200b\u663e\u793a\u200b\u540d\u200b&lt;/a&gt;</code></p> <p>\u200b\u4f7f\u7528\u200b\u5c16\u62ec\u53f7\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u65b9\u4fbf\u200b\u5730\u200b\u628a\u200bURL\u200b\u6216\u8005\u200bemail\u200b\u5730\u5740\u200b\u53d8\u6210\u200b\u53ef\u200b\u70b9\u51fb\u200b\u7684\u200b\u94fe\u63a5\u200b\u3002 <code>&lt;abc@cde.a&gt;</code> abc@cde.a</p> <p>\u200b\u5f15\u7528\u200b\u7c7b\u578b\u200b\u94fe\u63a5\u200b\u5206\u4e3a\u200b\u4e24\u200b\u90e8\u5206\u200b\uff1a</p> <ol> <li> <p>\u200b\u4e0e\u200b\u6587\u672c\u200b\u4fdd\u6301\u200b\u5185\u8054\u200b\u7684\u200b\u90e8\u5206\u200b    \u200b\u7b2c\u4e00\u7ec4\u200b\u65b9\u62ec\u53f7\u200b\u5305\u56f4\u200b\u5e94\u200b\u663e\u793a\u200b\u4e3a\u200b\u94fe\u63a5\u200b\u7684\u200b\u6587\u672c\u200b\u3002\u200b\u7b2c\u4e8c\u7ec4\u200b\u62ec\u53f7\u200b\u663e\u793a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6807\u7b7e\u200b\uff0c\u200b\u8be5\u200b\u6807\u7b7e\u200b\u7528\u4e8e\u200b\u6307\u5411\u200b\u60a8\u200b\u5b58\u50a8\u200b\u5728\u200b\u6587\u6863\u200b\u5176\u4ed6\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u94fe\u63a5\u200b\u3002 \u200b\u94fe\u63a5\u200b1 \u200b\u94fe\u63a5\u200b2</p> <pre><code>[\u200b\u94fe\u63a5\u200b1][1]   [\u200b\u94fe\u63a5\u200b2][2]\n</code></pre> </li> <li> <p>\u200b\u5b58\u50a8\u200b\u5728\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u5176\u4ed6\u200b\u4f4d\u7f6e\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u4ee5\u200b\u4f7f\u200b\u6587\u672c\u200b\u6613\u4e8e\u200b\u9605\u8bfb\u200b\u3002</p> <pre><code>[1]: https://www.baidu.com\n[2]: linux.md\n</code></pre> </li> </ol>"},{"location":"Tools/markdown.html#markdown-table","title":"Markdown table","text":"Syntax Description Header Title Paragraph Text"},{"location":"Tools/markdown.html#mkdocs","title":"mkdocs \u200b\u63d2\u4ef6","text":""},{"location":"Tools/markdown.html#amonition","title":"amonition","text":"<p>\u200b\u901a\u8fc7\u200b <code>!!!</code> \u200b\u8bed\u6cd5\u200b\u6765\u200b\u7ed9\u5b9a\u200b\u4e00\u4e9b\u200b\u544a\u8b66\u200b</p> <pre><code>!!! note Name\n</code></pre> <p>\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u98ce\u683c\u200b\uff0c\u200b\u7f51\u9875\u200b\u9884\u89c8\u200b\u548c\u200bvscode\u200b\u4e0d\u592a\u200b\u4e00\u6837\u200b</p> <p>Warning</p> <p>Danger</p> <p>Bug</p> <p>Note</p> <p>Abstract</p> <p>Info</p> <p>Example</p> <p>Hint</p> <p>Tip</p> <p>Success</p> <p>Question</p> <p>Quote</p> <p>If use <code>???</code>, then the block can be folded( only visible in browser, not in vscode preview).</p> Note <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum</p> <p>If use <code>inline end</code> ,then the next line will be inlined to right side.</p> <p>The next next line will not be inlined, for example this line.</p> <p>Lorem ipsum</p> <p>If use <code>inline</code> ,then the next line will be inlined to left side.</p> <p>\u200b\u4f7f\u7528\u200b <code>===</code> \u200b\u80fd\u591f\u200b\u8fdb\u200b\u6807\u7b7e\u200b\u5207\u6362\u200b\uff0c\u200b\u5192\u53f7\u200b\u524d\u200b\u7684\u200b\u5185\u5bb9\u200b\u6307\u5b9a\u200b\u56fe\u6807\u200b</p>  Tab A Tab B <p>Content of part A</p> <p>Content of part B</p>"},{"location":"Leetcode/tags.html","title":"Tags","text":"<p>Following is a list of relevant tags:</p>"},{"location":"Leetcode/tags.html#array","title":"Array","text":"<ul> <li>1207. Unique Number of Occurrences</li> <li>134. Gas Station</li> <li>380. Insert Delete GetRandom O(1)</li> </ul>"},{"location":"Leetcode/tags.html#bst","title":"BST","text":"<ul> <li>230. Kth Smallest Element in a BST</li> <li>530. Minimum Absolute Difference in BST</li> <li>98. Validate Binary Search Tree</li> </ul>"},{"location":"Leetcode/tags.html#easy","title":"Easy","text":"<ul> <li>1207. Unique Number of Occurrences</li> <li>530. Minimum Absolute Difference in BST</li> </ul>"},{"location":"Leetcode/tags.html#greedy","title":"Greedy","text":"<ul> <li>134. Gas Station</li> </ul>"},{"location":"Leetcode/tags.html#hash-table","title":"Hash Table","text":"<ul> <li>1207. Unique Number of Occurrences</li> <li>380. Insert Delete GetRandom O(1)</li> </ul>"},{"location":"Leetcode/tags.html#medium","title":"Medium","text":"<ul> <li>134. Gas Station</li> <li>230. Kth Smallest Element in a BST</li> <li>380. Insert Delete GetRandom O(1)</li> <li>98. Validate Binary Search Tree</li> </ul>"},{"location":"Leetcode/tags.html#tree","title":"Tree","text":"<ul> <li>230. Kth Smallest Element in a BST</li> <li>530. Minimum Absolute Difference in BST</li> <li>98. Validate Binary Search Tree</li> </ul>"}]}